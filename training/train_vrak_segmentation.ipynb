{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the library that can be useful to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common library\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "# Preparation of the dataset\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# Visualisation of the segmentation \n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# Configure the model\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# Data augmentation\n",
    "from detectron2.data import detection_utils as d_utils\n",
    "from detectron2.data import build_detection_train_loader\n",
    "from detectron2.data import transforms as T\n",
    "from detectron2.data import DatasetMapper\n",
    "\n",
    "\n",
    "# Evaluate the model \n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader\n",
    "\n",
    "# Train\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "from typing import List, Optional, Union\n",
    "import torch\n",
    "\n",
    "from detectron2.config import configurable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need first to register the dataset in COCO format, we could have used a customized function for any other dataset format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace by your path to images and json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JSON = \"datasets/train/annotations/instances_default.json\"\n",
    "TRAIN_IMAGES = \"datasets/train/images\"\n",
    "TEST_JSON = \"datasets/test/annotations/instances_default.json\"\n",
    "TEST_IMAGES = \"datasets/test/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_coco_instances(\n",
    "    name=\"bloc_segmentation_train\", \n",
    "    metadata={}, \n",
    "    json_file=TRAIN_JSON, \n",
    "    image_root=TRAIN_IMAGES\n",
    ")\n",
    "\n",
    "register_coco_instances(\n",
    "    name=\"bloc_segmentation_test\", \n",
    "    metadata={}, \n",
    "    json_file=TEST_JSON, \n",
    "    image_root=TEST_IMAGES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can change all the hyperparameters, you can find the differents architectures [here](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "DATA_SET_NAME = \"bloc_segmentation\"\n",
    "ARCHITECTURE = \"mask_rcnn_R_101_FPN_3x\"\n",
    "CONFIG_FILE_PATH = f\"COCO-InstanceSegmentation/{ARCHITECTURE}.yaml\"\n",
    "MAX_ITER = 2000\n",
    "EVAL_PERIOD = 100\n",
    "BASE_LR = 0.001\n",
    "NUM_CLASSES = 1\n",
    "CHECKPOINT = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create automatically the folder where model are going to be saved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output dir \n",
    "OUTPUT_DIR_PATH = os.path.join(\n",
    "    DATA_SET_NAME, \n",
    "    ARCHITECTURE, \n",
    "    datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    ")\n",
    "os.makedirs(OUTPUT_DIR_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(CONFIG_FILE_PATH)) # Get the arch\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(CONFIG_FILE_PATH) # Get the weights\n",
    "cfg.DATASETS.TRAIN = (\"bloc_segmentation_train\",) \n",
    "cfg.DATASETS.TEST = (\"bloc_segmentation_test\",)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64 # Come from the RPN process, the RPN uses a sliding window\n",
    "cfg.TEST.EVAL_PERIOD = EVAL_PERIOD # Evaluation period\n",
    "cfg.DATALOADER.NUM_WORKERS = 2 # How many core of the cpu are going to be used for the process\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1 # Number of training exemple in one iteration\n",
    "cfg.INPUT.MASK_FORMAT='bitmask' # Format of segmentaion mask \n",
    "cfg.SOLVER.BASE_LR = BASE_LR # Learning rate\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER # Number of iteration\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = NUM_CLASSES # Number of classes \n",
    "cfg.OUTPUT_DIR = OUTPUT_DIR_PATH # Output dir\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = CHECKPOINT # Checkpoint period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We overwrite the default Dataset mapper. The dataset mapper is used by the data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can modify the custom_augmentation if you do data augmentation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDatasetMapper(DatasetMapper):\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        is_train: bool,\n",
    "        augmentations: List[Union[T.Augmentation, T.Transform]],\n",
    "        image_format: str,\n",
    "        use_instance_mask: bool = False,\n",
    "        use_keypoint: bool = False,\n",
    "        instance_mask_format: str = \"polygon\",\n",
    "        keypoint_hflip_indices: Optional[np.ndarray] = None,\n",
    "        precomputed_proposal_topk: Optional[int] = None,\n",
    "        recompute_boxes: bool = False,\n",
    "    ):\n",
    "        \n",
    "        custom_augmentations = [\n",
    "            T.RandomApply(T.RandomFlip(prob=0.4, horizontal=False, vertical=True), prob=0.5),\n",
    "            T.RandomApply(T.RandomBrightness(0.8, 1.2), prob=0.5),\n",
    "            T.RandomApply(T.RandomContrast(0.6, 1.4), prob=0.5),\n",
    "            T.RandomApply(T.RandomSaturation(0.8, 1.2), prob=0.5)\n",
    "        ]\n",
    "        augmentations.extend(custom_augmentations)\n",
    "\n",
    "        super().__init__(\n",
    "            is_train=is_train,\n",
    "            augmentations=augmentations,\n",
    "            image_format=image_format,\n",
    "            use_instance_mask=use_instance_mask,\n",
    "            use_keypoint=use_keypoint,\n",
    "            instance_mask_format=instance_mask_format,\n",
    "            keypoint_hflip_indices=keypoint_hflip_indices,\n",
    "            precomputed_proposal_topk=precomputed_proposal_topk,\n",
    "            recompute_boxes=recompute_boxes\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, is_train: bool = True):\n",
    "        augs = d_utils.build_augmentation(cfg, is_train)\n",
    "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
    "            augs.insert(0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE))\n",
    "            recompute_boxes = cfg.MODEL.MASK_ON\n",
    "        else:\n",
    "            recompute_boxes = False\n",
    "\n",
    "        ret = {\n",
    "            \"is_train\": is_train,\n",
    "            \"augmentations\": augs,\n",
    "            \"image_format\": cfg.INPUT.FORMAT,\n",
    "            \"use_instance_mask\": cfg.MODEL.MASK_ON,\n",
    "            \"instance_mask_format\": cfg.INPUT.MASK_FORMAT,\n",
    "            \"use_keypoint\": cfg.MODEL.KEYPOINT_ON,\n",
    "            \"recompute_boxes\": recompute_boxes,\n",
    "        }\n",
    "\n",
    "        if cfg.MODEL.KEYPOINT_ON:\n",
    "            ret[\"keypoint_hflip_indices\"] = d_utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
    "\n",
    "        if cfg.MODEL.LOAD_PROPOSALS:\n",
    "            ret[\"precomputed_proposal_topk\"] = (\n",
    "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
    "                if is_train\n",
    "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
    "            )\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to create a new trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        return build_detection_train_loader(cfg, mapper=CustomDatasetMapper(cfg, is_train=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without data augmentation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/18 16:13:15 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[07/18 16:13:15 d2.data.datasets.coco]: \u001b[0mLoaded 109 images in COCO format from datasets/train/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:13:15 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 109 images left.\n",
      "\u001b[32m[07/18 16:13:15 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    bloc    | 8841         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/18 16:13:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/18 16:13:15 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/18 16:13:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:13:15 d2.data.common]: \u001b[0mSerializing 109 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:13:15 d2.data.common]: \u001b[0mSerialized dataset takes 3.25 MiB\n",
      "\u001b[32m[07/18 16:13:15 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:13:15 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[07/18 16:13:15 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/18 16:13:15 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\myenv\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/18 16:13:31 d2.utils.events]: \u001b[0m eta: 0:09:38  iter: 19  total_loss: 5.891  loss_cls: 0.6813  loss_box_reg: 0.6393  loss_mask: 0.696  loss_rpn_cls: 3.647  loss_rpn_loc: 0.2639    time: 0.5565  last_time: 0.2731  data_time: 0.5273  last_data_time: 0.0032   lr: 1.9981e-05  max_mem: 2366M\n",
      "\u001b[32m[07/18 16:13:46 d2.utils.events]: \u001b[0m eta: 0:10:33  iter: 39  total_loss: 2.611  loss_cls: 0.5902  loss_box_reg: 0.7096  loss_mask: 0.6691  loss_rpn_cls: 0.4023  loss_rpn_loc: 0.156    time: 0.6623  last_time: 0.2725  data_time: 0.5090  last_data_time: 0.0013   lr: 3.9961e-05  max_mem: 2635M\n",
      "\u001b[32m[07/18 16:14:01 d2.utils.events]: \u001b[0m eta: 0:10:53  iter: 59  total_loss: 2.149  loss_cls: 0.542  loss_box_reg: 0.6954  loss_mask: 0.6095  loss_rpn_cls: 0.1477  loss_rpn_loc: 0.1542    time: 0.6942  last_time: 1.3165  data_time: 0.5087  last_data_time: 1.0960   lr: 5.9941e-05  max_mem: 2898M\n",
      "\u001b[32m[07/18 16:14:21 d2.utils.events]: \u001b[0m eta: 0:10:47  iter: 79  total_loss: 2.208  loss_cls: 0.5082  loss_box_reg: 0.6765  loss_mask: 0.5612  loss_rpn_cls: 0.1794  loss_rpn_loc: 0.1675    time: 0.7607  last_time: 0.2570  data_time: 0.6867  last_data_time: 0.0127   lr: 7.9921e-05  max_mem: 3784M\n",
      "\u001b[32m[07/18 16:14:34 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:14:34 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    bloc    | 2107         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/18 16:14:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:14:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:14:34 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:14:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:14:34 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:14:34 d2.utils.events]: \u001b[0m eta: 0:10:40  iter: 99  total_loss: 2.035  loss_cls: 0.485  loss_box_reg: 0.628  loss_mask: 0.4692  loss_rpn_cls: 0.1844  loss_rpn_loc: 0.1474    time: 0.7437  last_time: 0.2473  data_time: 0.4201  last_data_time: 0.0149   lr: 9.9901e-05  max_mem: 3784M\n",
      "\u001b[32m[07/18 16:14:48 d2.utils.events]: \u001b[0m eta: 0:11:06  iter: 119  total_loss: 1.781  loss_cls: 0.4589  loss_box_reg: 0.6532  loss_mask: 0.4223  loss_rpn_cls: 0.128  loss_rpn_loc: 0.1553    time: 0.7335  last_time: 0.2562  data_time: 0.4239  last_data_time: 0.0093   lr: 0.00011988  max_mem: 3784M\n",
      "\u001b[32m[07/18 16:15:04 d2.utils.events]: \u001b[0m eta: 0:11:08  iter: 139  total_loss: 1.608  loss_cls: 0.4183  loss_box_reg: 0.5574  loss_mask: 0.367  loss_rpn_cls: 0.1135  loss_rpn_loc: 0.154    time: 0.7474  last_time: 0.2237  data_time: 0.5609  last_data_time: 0.0108   lr: 0.00013986  max_mem: 3784M\n",
      "\u001b[32m[07/18 16:15:18 d2.utils.events]: \u001b[0m eta: 0:11:01  iter: 159  total_loss: 1.665  loss_cls: 0.3965  loss_box_reg: 0.6441  loss_mask: 0.3357  loss_rpn_cls: 0.1098  loss_rpn_loc: 0.1485    time: 0.7390  last_time: 0.2684  data_time: 0.4372  last_data_time: 0.0087   lr: 0.00015984  max_mem: 3784M\n",
      "\u001b[32m[07/18 16:15:36 d2.utils.events]: \u001b[0m eta: 0:10:54  iter: 179  total_loss: 1.611  loss_cls: 0.3846  loss_box_reg: 0.6063  loss_mask: 0.2947  loss_rpn_cls: 0.1202  loss_rpn_loc: 0.1673    time: 0.7571  last_time: 1.6205  data_time: 0.6046  last_data_time: 1.4060   lr: 0.00017982  max_mem: 3822M\n",
      "\u001b[32m[07/18 16:15:49 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:15:49 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:15:49 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:15:49 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:15:49 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:15:49 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:15:49 d2.utils.events]: \u001b[0m eta: 0:10:14  iter: 199  total_loss: 1.465  loss_cls: 0.3797  loss_box_reg: 0.5271  loss_mask: 0.268  loss_rpn_cls: 0.1318  loss_rpn_loc: 0.1648    time: 0.7454  last_time: 0.2326  data_time: 0.3709  last_data_time: 0.0068   lr: 0.0001998  max_mem: 3822M\n",
      "\u001b[32m[07/18 16:16:02 d2.utils.events]: \u001b[0m eta: 0:10:53  iter: 219  total_loss: 1.233  loss_cls: 0.3089  loss_box_reg: 0.4792  loss_mask: 0.2149  loss_rpn_cls: 0.1033  loss_rpn_loc: 0.116    time: 0.7363  last_time: 0.2704  data_time: 0.3357  last_data_time: 0.0136   lr: 0.00021978  max_mem: 3822M\n",
      "\u001b[32m[07/18 16:16:15 d2.utils.events]: \u001b[0m eta: 0:10:42  iter: 239  total_loss: 1.299  loss_cls: 0.325  loss_box_reg: 0.4877  loss_mask: 0.2459  loss_rpn_cls: 0.06453  loss_rpn_loc: 0.146    time: 0.7307  last_time: 0.4063  data_time: 0.3812  last_data_time: 0.1700   lr: 0.00023976  max_mem: 3822M\n",
      "\u001b[32m[07/18 16:16:34 d2.utils.events]: \u001b[0m eta: 0:10:35  iter: 259  total_loss: 1.225  loss_cls: 0.302  loss_box_reg: 0.4368  loss_mask: 0.2346  loss_rpn_cls: 0.1085  loss_rpn_loc: 0.1196    time: 0.7459  last_time: 3.6995  data_time: 0.6175  last_data_time: 2.6722   lr: 0.00025974  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:16:44 d2.utils.events]: \u001b[0m eta: 0:10:20  iter: 279  total_loss: 1.11  loss_cls: 0.31  loss_box_reg: 0.4025  loss_mask: 0.203  loss_rpn_cls: 0.08397  loss_rpn_loc: 0.1287    time: 0.7276  last_time: 0.3136  data_time: 0.2285  last_data_time: 0.0905   lr: 0.00027972  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:17:00 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:17:00 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:17:00 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:17:00 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:17:00 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:17:00 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:17:00 d2.utils.events]: \u001b[0m eta: 0:10:18  iter: 299  total_loss: 1.185  loss_cls: 0.3197  loss_box_reg: 0.3755  loss_mask: 0.2113  loss_rpn_cls: 0.07812  loss_rpn_loc: 0.134    time: 0.7349  last_time: 0.2384  data_time: 0.5633  last_data_time: 0.0087   lr: 0.0002997  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:17:14 d2.utils.events]: \u001b[0m eta: 0:10:12  iter: 319  total_loss: 1.277  loss_cls: 0.3336  loss_box_reg: 0.396  loss_mask: 0.2291  loss_rpn_cls: 0.09149  loss_rpn_loc: 0.1305    time: 0.7311  last_time: 0.3234  data_time: 0.4195  last_data_time: 0.0060   lr: 0.00031968  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:17:28 d2.utils.events]: \u001b[0m eta: 0:10:04  iter: 339  total_loss: 1.128  loss_cls: 0.2926  loss_box_reg: 0.3813  loss_mask: 0.205  loss_rpn_cls: 0.116  loss_rpn_loc: 0.1452    time: 0.7295  last_time: 0.7390  data_time: 0.4300  last_data_time: 0.4751   lr: 0.00033966  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:17:44 d2.utils.events]: \u001b[0m eta: 0:09:58  iter: 359  total_loss: 1.058  loss_cls: 0.2635  loss_box_reg: 0.349  loss_mask: 0.2039  loss_rpn_cls: 0.07302  loss_rpn_loc: 0.1243    time: 0.7327  last_time: 0.2602  data_time: 0.5188  last_data_time: 0.0062   lr: 0.00035964  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:18:02 d2.utils.events]: \u001b[0m eta: 0:09:54  iter: 379  total_loss: 1.228  loss_cls: 0.3182  loss_box_reg: 0.3823  loss_mask: 0.221  loss_rpn_cls: 0.08493  loss_rpn_loc: 0.1275    time: 0.7415  last_time: 0.7983  data_time: 0.6167  last_data_time: 0.5284   lr: 0.00037962  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:18:16 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:18:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:18:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:18:16 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:18:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:18:16 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:18:16 d2.utils.events]: \u001b[0m eta: 0:09:43  iter: 399  total_loss: 0.9552  loss_cls: 0.267  loss_box_reg: 0.3268  loss_mask: 0.1905  loss_rpn_cls: 0.05618  loss_rpn_loc: 0.135    time: 0.7388  last_time: 0.3651  data_time: 0.3867  last_data_time: 0.0186   lr: 0.0003996  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:18:28 d2.utils.events]: \u001b[0m eta: 0:09:36  iter: 419  total_loss: 1.035  loss_cls: 0.2909  loss_box_reg: 0.3541  loss_mask: 0.1977  loss_rpn_cls: 0.06659  loss_rpn_loc: 0.1186    time: 0.7335  last_time: 0.3155  data_time: 0.3181  last_data_time: 0.0077   lr: 0.00041958  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:18:44 d2.utils.events]: \u001b[0m eta: 0:09:38  iter: 439  total_loss: 1.035  loss_cls: 0.294  loss_box_reg: 0.3124  loss_mask: 0.2182  loss_rpn_cls: 0.0808  loss_rpn_loc: 0.1419    time: 0.7358  last_time: 0.2711  data_time: 0.3957  last_data_time: 0.0111   lr: 0.00043956  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:18:57 d2.utils.events]: \u001b[0m eta: 0:10:06  iter: 459  total_loss: 0.9755  loss_cls: 0.2912  loss_box_reg: 0.2712  loss_mask: 0.1894  loss_rpn_cls: 0.06187  loss_rpn_loc: 0.1164    time: 0.7316  last_time: 1.0717  data_time: 0.2838  last_data_time: 0.6868   lr: 0.00045954  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:19:13 d2.utils.events]: \u001b[0m eta: 0:10:42  iter: 479  total_loss: 1.018  loss_cls: 0.2863  loss_box_reg: 0.3549  loss_mask: 0.2044  loss_rpn_cls: 0.05796  loss_rpn_loc: 0.1229    time: 0.7355  last_time: 0.2855  data_time: 0.4830  last_data_time: 0.0183   lr: 0.00047952  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:19:28 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:19:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:19:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:19:28 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:19:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:19:28 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:19:28 d2.utils.events]: \u001b[0m eta: 0:10:22  iter: 499  total_loss: 1.026  loss_cls: 0.2715  loss_box_reg: 0.3237  loss_mask: 0.2322  loss_rpn_cls: 0.05931  loss_rpn_loc: 0.1137    time: 0.7353  last_time: 0.3363  data_time: 0.4044  last_data_time: 0.0035   lr: 0.0004995  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:19:43 d2.utils.events]: \u001b[0m eta: 0:10:22  iter: 519  total_loss: 0.9098  loss_cls: 0.2543  loss_box_reg: 0.3187  loss_mask: 0.1997  loss_rpn_cls: 0.05335  loss_rpn_loc: 0.1127    time: 0.7356  last_time: 1.3399  data_time: 0.4115  last_data_time: 0.9848   lr: 0.00051948  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:19:57 d2.utils.events]: \u001b[0m eta: 0:10:13  iter: 539  total_loss: 0.9703  loss_cls: 0.26  loss_box_reg: 0.3645  loss_mask: 0.2008  loss_rpn_cls: 0.07531  loss_rpn_loc: 0.1187    time: 0.7344  last_time: 0.4843  data_time: 0.3674  last_data_time: 0.1634   lr: 0.00053946  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:20:13 d2.utils.events]: \u001b[0m eta: 0:10:05  iter: 559  total_loss: 0.9833  loss_cls: 0.2559  loss_box_reg: 0.3415  loss_mask: 0.1996  loss_rpn_cls: 0.06892  loss_rpn_loc: 0.1306    time: 0.7364  last_time: 0.5003  data_time: 0.4707  last_data_time: 0.1325   lr: 0.00055944  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:20:29 d2.utils.events]: \u001b[0m eta: 0:10:01  iter: 579  total_loss: 0.931  loss_cls: 0.2334  loss_box_reg: 0.2813  loss_mask: 0.1746  loss_rpn_cls: 0.0457  loss_rpn_loc: 0.1282    time: 0.7393  last_time: 1.8653  data_time: 0.4862  last_data_time: 1.4835   lr: 0.00057942  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:20:46 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:20:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:20:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:20:46 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:20:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:20:46 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:20:46 d2.utils.events]: \u001b[0m eta: 0:09:42  iter: 599  total_loss: 0.9157  loss_cls: 0.2828  loss_box_reg: 0.2822  loss_mask: 0.1885  loss_rpn_cls: 0.06527  loss_rpn_loc: 0.1129    time: 0.7432  last_time: 0.2728  data_time: 0.4542  last_data_time: 0.0110   lr: 0.0005994  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:21:01 d2.utils.events]: \u001b[0m eta: 0:09:40  iter: 619  total_loss: 1.004  loss_cls: 0.3004  loss_box_reg: 0.3003  loss_mask: 0.2004  loss_rpn_cls: 0.07179  loss_rpn_loc: 0.1105    time: 0.7430  last_time: 2.1035  data_time: 0.4210  last_data_time: 1.7306   lr: 0.00061938  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:21:17 d2.utils.events]: \u001b[0m eta: 0:09:25  iter: 639  total_loss: 0.9481  loss_cls: 0.29  loss_box_reg: 0.2703  loss_mask: 0.1835  loss_rpn_cls: 0.06393  loss_rpn_loc: 0.1118    time: 0.7453  last_time: 0.3342  data_time: 0.5453  last_data_time: 0.0051   lr: 0.00063936  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:21:31 d2.utils.events]: \u001b[0m eta: 0:09:17  iter: 659  total_loss: 0.9691  loss_cls: 0.2659  loss_box_reg: 0.3227  loss_mask: 0.1959  loss_rpn_cls: 0.06336  loss_rpn_loc: 0.1231    time: 0.7431  last_time: 0.3079  data_time: 0.4127  last_data_time: 0.0067   lr: 0.00065934  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:21:46 d2.utils.events]: \u001b[0m eta: 0:09:03  iter: 679  total_loss: 0.9547  loss_cls: 0.2723  loss_box_reg: 0.2926  loss_mask: 0.1977  loss_rpn_cls: 0.07023  loss_rpn_loc: 0.1111    time: 0.7429  last_time: 0.2585  data_time: 0.4409  last_data_time: 0.0126   lr: 0.00067932  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:22:02 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:22:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:22:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:22:02 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:22:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:22:02 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:22:02 d2.utils.events]: \u001b[0m eta: 0:09:07  iter: 699  total_loss: 1.021  loss_cls: 0.2714  loss_box_reg: 0.3398  loss_mask: 0.1953  loss_rpn_cls: 0.05246  loss_rpn_loc: 0.1349    time: 0.7454  last_time: 0.1929  data_time: 0.6028  last_data_time: 0.0178   lr: 0.0006993  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:22:15 d2.utils.events]: \u001b[0m eta: 0:08:52  iter: 719  total_loss: 0.9418  loss_cls: 0.272  loss_box_reg: 0.3011  loss_mask: 0.1881  loss_rpn_cls: 0.04445  loss_rpn_loc: 0.1158    time: 0.7429  last_time: 0.2109  data_time: 0.4239  last_data_time: 0.0068   lr: 0.00071928  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:22:29 d2.utils.events]: \u001b[0m eta: 0:08:41  iter: 739  total_loss: 0.9861  loss_cls: 0.2428  loss_box_reg: 0.3007  loss_mask: 0.1777  loss_rpn_cls: 0.04668  loss_rpn_loc: 0.1327    time: 0.7413  last_time: 0.1855  data_time: 0.4699  last_data_time: 0.0041   lr: 0.00073926  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:22:44 d2.utils.events]: \u001b[0m eta: 0:08:32  iter: 759  total_loss: 0.989  loss_cls: 0.3019  loss_box_reg: 0.2913  loss_mask: 0.1785  loss_rpn_cls: 0.08933  loss_rpn_loc: 0.1396    time: 0.7408  last_time: 1.2078  data_time: 0.4931  last_data_time: 0.9866   lr: 0.00075924  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:22:58 d2.utils.events]: \u001b[0m eta: 0:08:24  iter: 779  total_loss: 0.9136  loss_cls: 0.2368  loss_box_reg: 0.3065  loss_mask: 0.1944  loss_rpn_cls: 0.05113  loss_rpn_loc: 0.1366    time: 0.7404  last_time: 0.8432  data_time: 0.4942  last_data_time: 0.6418   lr: 0.00077922  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:23:09 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:23:09 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:23:09 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:23:09 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:23:09 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:23:09 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:23:09 d2.utils.events]: \u001b[0m eta: 0:08:07  iter: 799  total_loss: 0.8527  loss_cls: 0.2556  loss_box_reg: 0.2639  loss_mask: 0.1793  loss_rpn_cls: 0.04282  loss_rpn_loc: 0.1317    time: 0.7360  last_time: 0.2569  data_time: 0.3451  last_data_time: 0.0103   lr: 0.0007992  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:23:23 d2.utils.events]: \u001b[0m eta: 0:07:59  iter: 819  total_loss: 0.9149  loss_cls: 0.249  loss_box_reg: 0.2889  loss_mask: 0.1796  loss_rpn_cls: 0.0641  loss_rpn_loc: 0.1347    time: 0.7342  last_time: 0.2724  data_time: 0.4221  last_data_time: 0.0115   lr: 0.00081918  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:23:39 d2.utils.events]: \u001b[0m eta: 0:07:51  iter: 839  total_loss: 0.8469  loss_cls: 0.267  loss_box_reg: 0.3033  loss_mask: 0.1516  loss_rpn_cls: 0.05459  loss_rpn_loc: 0.1207    time: 0.7365  last_time: 0.7497  data_time: 0.5437  last_data_time: 0.5290   lr: 0.00083916  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:23:52 d2.utils.events]: \u001b[0m eta: 0:07:42  iter: 859  total_loss: 0.9698  loss_cls: 0.2358  loss_box_reg: 0.321  loss_mask: 0.1906  loss_rpn_cls: 0.03677  loss_rpn_loc: 0.1263    time: 0.7338  last_time: 0.2914  data_time: 0.3611  last_data_time: 0.0362   lr: 0.00085914  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:24:10 d2.utils.events]: \u001b[0m eta: 0:07:35  iter: 879  total_loss: 0.893  loss_cls: 0.2354  loss_box_reg: 0.2715  loss_mask: 0.1822  loss_rpn_cls: 0.05855  loss_rpn_loc: 0.1227    time: 0.7382  last_time: 0.3301  data_time: 0.6178  last_data_time: 0.0056   lr: 0.00087912  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:24:25 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:24:25 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:24:25 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:24:25 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:24:25 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:24:25 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:24:25 d2.utils.events]: \u001b[0m eta: 0:07:29  iter: 899  total_loss: 0.8076  loss_cls: 0.2467  loss_box_reg: 0.2576  loss_mask: 0.1573  loss_rpn_cls: 0.03791  loss_rpn_loc: 0.1155    time: 0.7385  last_time: 0.6233  data_time: 0.4488  last_data_time: 0.3708   lr: 0.0008991  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:24:39 d2.utils.events]: \u001b[0m eta: 0:07:24  iter: 919  total_loss: 0.9035  loss_cls: 0.2216  loss_box_reg: 0.2905  loss_mask: 0.1748  loss_rpn_cls: 0.03355  loss_rpn_loc: 0.1199    time: 0.7376  last_time: 2.1865  data_time: 0.4246  last_data_time: 1.8020   lr: 0.00091908  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:24:55 d2.utils.events]: \u001b[0m eta: 0:07:16  iter: 939  total_loss: 1.118  loss_cls: 0.2925  loss_box_reg: 0.3788  loss_mask: 0.208  loss_rpn_cls: 0.07218  loss_rpn_loc: 0.1599    time: 0.7385  last_time: 0.8209  data_time: 0.5472  last_data_time: 0.6002   lr: 0.00093906  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:25:07 d2.utils.events]: \u001b[0m eta: 0:07:03  iter: 959  total_loss: 0.8812  loss_cls: 0.2426  loss_box_reg: 0.2842  loss_mask: 0.1844  loss_rpn_cls: 0.0503  loss_rpn_loc: 0.1228    time: 0.7359  last_time: 1.2165  data_time: 0.3882  last_data_time: 1.0456   lr: 0.00095904  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:25:20 d2.utils.events]: \u001b[0m eta: 0:06:55  iter: 979  total_loss: 0.9669  loss_cls: 0.2704  loss_box_reg: 0.2979  loss_mask: 0.1907  loss_rpn_cls: 0.04149  loss_rpn_loc: 0.1348    time: 0.7336  last_time: 1.3644  data_time: 0.4066  last_data_time: 1.1241   lr: 0.00097902  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:25:33 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:25:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:25:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:25:33 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:25:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:25:33 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:25:33 d2.utils.events]: \u001b[0m eta: 0:06:53  iter: 999  total_loss: 0.8279  loss_cls: 0.2194  loss_box_reg: 0.3028  loss_mask: 0.1692  loss_rpn_cls: 0.04272  loss_rpn_loc: 0.1326    time: 0.7318  last_time: 0.2242  data_time: 0.4371  last_data_time: 0.0085   lr: 0.000999  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:25:44 d2.utils.events]: \u001b[0m eta: 0:06:45  iter: 1019  total_loss: 0.9561  loss_cls: 0.2775  loss_box_reg: 0.2883  loss_mask: 0.2003  loss_rpn_cls: 0.05026  loss_rpn_loc: 0.1459    time: 0.7279  last_time: 0.2155  data_time: 0.2957  last_data_time: 0.0083   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:25:57 d2.utils.events]: \u001b[0m eta: 0:06:40  iter: 1039  total_loss: 0.7545  loss_cls: 0.2217  loss_box_reg: 0.2691  loss_mask: 0.1573  loss_rpn_cls: 0.03846  loss_rpn_loc: 0.1182    time: 0.7266  last_time: 0.3024  data_time: 0.4192  last_data_time: 0.0087   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:26:09 d2.utils.events]: \u001b[0m eta: 0:06:30  iter: 1059  total_loss: 0.8616  loss_cls: 0.23  loss_box_reg: 0.2871  loss_mask: 0.1695  loss_rpn_cls: 0.03132  loss_rpn_loc: 0.1126    time: 0.7241  last_time: 0.2036  data_time: 0.3736  last_data_time: 0.0089   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:26:23 d2.utils.events]: \u001b[0m eta: 0:06:22  iter: 1079  total_loss: 0.9211  loss_cls: 0.2437  loss_box_reg: 0.3352  loss_mask: 0.1867  loss_rpn_cls: 0.04452  loss_rpn_loc: 0.1072    time: 0.7237  last_time: 0.2724  data_time: 0.4888  last_data_time: 0.0102   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:26:36 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:26:36 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:26:36 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:26:36 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:26:36 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:26:36 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:26:36 d2.utils.events]: \u001b[0m eta: 0:06:14  iter: 1099  total_loss: 0.9294  loss_cls: 0.2687  loss_box_reg: 0.2772  loss_mask: 0.2106  loss_rpn_cls: 0.04924  loss_rpn_loc: 0.1109    time: 0.7223  last_time: 0.6198  data_time: 0.4272  last_data_time: 0.3501   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:26:52 d2.utils.events]: \u001b[0m eta: 0:06:05  iter: 1119  total_loss: 0.8898  loss_cls: 0.235  loss_box_reg: 0.321  loss_mask: 0.1914  loss_rpn_cls: 0.03898  loss_rpn_loc: 0.1184    time: 0.7234  last_time: 0.6500  data_time: 0.5173  last_data_time: 0.4709   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:27:07 d2.utils.events]: \u001b[0m eta: 0:05:53  iter: 1139  total_loss: 0.8547  loss_cls: 0.2454  loss_box_reg: 0.2765  loss_mask: 0.1741  loss_rpn_cls: 0.0358  loss_rpn_loc: 0.12    time: 0.7237  last_time: 0.4087  data_time: 0.4687  last_data_time: 0.1295   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:27:19 d2.utils.events]: \u001b[0m eta: 0:05:45  iter: 1159  total_loss: 0.84  loss_cls: 0.1903  loss_box_reg: 0.2623  loss_mask: 0.1665  loss_rpn_cls: 0.04877  loss_rpn_loc: 0.1241    time: 0.7221  last_time: 0.2563  data_time: 0.4257  last_data_time: 0.0087   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:27:32 d2.utils.events]: \u001b[0m eta: 0:05:35  iter: 1179  total_loss: 0.8204  loss_cls: 0.1968  loss_box_reg: 0.2466  loss_mask: 0.1584  loss_rpn_cls: 0.02957  loss_rpn_loc: 0.1211    time: 0.7205  last_time: 0.2327  data_time: 0.3948  last_data_time: 0.0083   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:27:45 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:27:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:27:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:27:45 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:27:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:27:45 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:27:45 d2.utils.events]: \u001b[0m eta: 0:05:36  iter: 1199  total_loss: 0.7781  loss_cls: 0.2186  loss_box_reg: 0.2699  loss_mask: 0.1498  loss_rpn_cls: 0.04332  loss_rpn_loc: 0.1333    time: 0.7191  last_time: 0.5957  data_time: 0.4185  last_data_time: 0.3844   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:27:58 d2.utils.events]: \u001b[0m eta: 0:05:22  iter: 1219  total_loss: 0.8629  loss_cls: 0.2175  loss_box_reg: 0.3266  loss_mask: 0.2002  loss_rpn_cls: 0.02974  loss_rpn_loc: 0.1153    time: 0.7180  last_time: 0.2570  data_time: 0.4248  last_data_time: 0.0055   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:28:15 d2.utils.events]: \u001b[0m eta: 0:05:19  iter: 1239  total_loss: 0.9862  loss_cls: 0.2593  loss_box_reg: 0.321  loss_mask: 0.2045  loss_rpn_cls: 0.07193  loss_rpn_loc: 0.1534    time: 0.7207  last_time: 0.2646  data_time: 0.6335  last_data_time: 0.0048   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:28:28 d2.utils.events]: \u001b[0m eta: 0:05:11  iter: 1259  total_loss: 0.8322  loss_cls: 0.2301  loss_box_reg: 0.2744  loss_mask: 0.1718  loss_rpn_cls: 0.04589  loss_rpn_loc: 0.1216    time: 0.7194  last_time: 0.2327  data_time: 0.4192  last_data_time: 0.0115   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:28:40 d2.utils.events]: \u001b[0m eta: 0:05:05  iter: 1279  total_loss: 0.7758  loss_cls: 0.2079  loss_box_reg: 0.2349  loss_mask: 0.1527  loss_rpn_cls: 0.05647  loss_rpn_loc: 0.1174    time: 0.7174  last_time: 0.2182  data_time: 0.3569  last_data_time: 0.0094   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:28:53 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:28:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:28:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:28:53 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:28:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:28:53 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:28:53 d2.utils.events]: \u001b[0m eta: 0:04:54  iter: 1299  total_loss: 0.712  loss_cls: 0.1584  loss_box_reg: 0.2437  loss_mask: 0.1372  loss_rpn_cls: 0.04096  loss_rpn_loc: 0.1264    time: 0.7163  last_time: 0.2740  data_time: 0.4002  last_data_time: 0.0057   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:29:07 d2.utils.events]: \u001b[0m eta: 0:04:47  iter: 1319  total_loss: 0.9074  loss_cls: 0.2552  loss_box_reg: 0.2871  loss_mask: 0.2087  loss_rpn_cls: 0.0344  loss_rpn_loc: 0.1352    time: 0.7159  last_time: 0.2607  data_time: 0.4438  last_data_time: 0.0125   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:29:19 d2.utils.events]: \u001b[0m eta: 0:04:39  iter: 1339  total_loss: 0.8241  loss_cls: 0.2105  loss_box_reg: 0.2684  loss_mask: 0.1551  loss_rpn_cls: 0.03118  loss_rpn_loc: 0.1273    time: 0.7142  last_time: 0.1808  data_time: 0.3687  last_data_time: 0.0061   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:29:34 d2.utils.events]: \u001b[0m eta: 0:04:31  iter: 1359  total_loss: 0.7884  loss_cls: 0.1973  loss_box_reg: 0.2667  loss_mask: 0.177  loss_rpn_cls: 0.02071  loss_rpn_loc: 0.1104    time: 0.7151  last_time: 0.7756  data_time: 0.5539  last_data_time: 0.4944   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:29:49 d2.utils.events]: \u001b[0m eta: 0:04:23  iter: 1379  total_loss: 0.747  loss_cls: 0.1681  loss_box_reg: 0.257  loss_mask: 0.1575  loss_rpn_cls: 0.0309  loss_rpn_loc: 0.09988    time: 0.7152  last_time: 1.2054  data_time: 0.4856  last_data_time: 1.0195   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:30:01 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:30:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:30:01 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:30:01 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:30:01 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:30:01 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:30:01 d2.utils.events]: \u001b[0m eta: 0:04:16  iter: 1399  total_loss: 0.8274  loss_cls: 0.2493  loss_box_reg: 0.2621  loss_mask: 0.1674  loss_rpn_cls: 0.02914  loss_rpn_loc: 0.09543    time: 0.7139  last_time: 0.2111  data_time: 0.4042  last_data_time: 0.0074   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:30:15 d2.utils.events]: \u001b[0m eta: 0:04:09  iter: 1419  total_loss: 0.8422  loss_cls: 0.2291  loss_box_reg: 0.2876  loss_mask: 0.1913  loss_rpn_cls: 0.03061  loss_rpn_loc: 0.1173    time: 0.7130  last_time: 1.5884  data_time: 0.4356  last_data_time: 1.3490   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:30:29 d2.utils.events]: \u001b[0m eta: 0:03:57  iter: 1439  total_loss: 0.711  loss_cls: 0.2397  loss_box_reg: 0.2258  loss_mask: 0.1433  loss_rpn_cls: 0.01929  loss_rpn_loc: 0.1075    time: 0.7130  last_time: 0.2669  data_time: 0.4694  last_data_time: 0.0185   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:30:47 d2.utils.events]: \u001b[0m eta: 0:03:48  iter: 1459  total_loss: 1.086  loss_cls: 0.2739  loss_box_reg: 0.305  loss_mask: 0.2003  loss_rpn_cls: 0.05112  loss_rpn_loc: 0.1364    time: 0.7157  last_time: 0.1974  data_time: 0.6727  last_data_time: 0.0144   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:31:02 d2.utils.events]: \u001b[0m eta: 0:03:31  iter: 1479  total_loss: 0.805  loss_cls: 0.2308  loss_box_reg: 0.2487  loss_mask: 0.1728  loss_rpn_cls: 0.03964  loss_rpn_loc: 0.1035    time: 0.7159  last_time: 0.8461  data_time: 0.4881  last_data_time: 0.6233   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:31:15 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:31:15 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:31:15 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:31:15 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:31:15 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:31:15 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:31:15 d2.utils.events]: \u001b[0m eta: 0:03:23  iter: 1499  total_loss: 0.753  loss_cls: 0.1958  loss_box_reg: 0.2525  loss_mask: 0.1575  loss_rpn_cls: 0.04221  loss_rpn_loc: 0.1137    time: 0.7150  last_time: 0.5290  data_time: 0.4184  last_data_time: 0.2904   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:31:28 d2.utils.events]: \u001b[0m eta: 0:03:13  iter: 1519  total_loss: 0.6483  loss_cls: 0.2011  loss_box_reg: 0.2265  loss_mask: 0.14  loss_rpn_cls: 0.0203  loss_rpn_loc: 0.09386    time: 0.7143  last_time: 0.2442  data_time: 0.4287  last_data_time: 0.0107   lr: 0.001  max_mem: 4372M\n",
      "\u001b[32m[07/18 16:31:46 d2.utils.events]: \u001b[0m eta: 0:03:05  iter: 1539  total_loss: 0.7523  loss_cls: 0.1995  loss_box_reg: 0.2509  loss_mask: 0.1815  loss_rpn_cls: 0.03349  loss_rpn_loc: 0.1031    time: 0.7165  last_time: 1.1207  data_time: 0.6163  last_data_time: 0.8914   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:32:01 d2.utils.events]: \u001b[0m eta: 0:02:58  iter: 1559  total_loss: 0.8111  loss_cls: 0.2464  loss_box_reg: 0.2959  loss_mask: 0.1564  loss_rpn_cls: 0.03144  loss_rpn_loc: 0.1158    time: 0.7170  last_time: 0.2045  data_time: 0.5105  last_data_time: 0.0058   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:32:14 d2.utils.events]: \u001b[0m eta: 0:02:47  iter: 1579  total_loss: 0.7352  loss_cls: 0.208  loss_box_reg: 0.2573  loss_mask: 0.151  loss_rpn_cls: 0.03183  loss_rpn_loc: 0.1163    time: 0.7164  last_time: 0.2158  data_time: 0.4500  last_data_time: 0.0130   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:32:29 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:32:29 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:32:29 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:32:29 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:32:29 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:32:29 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:32:29 d2.utils.events]: \u001b[0m eta: 0:02:48  iter: 1599  total_loss: 0.8212  loss_cls: 0.233  loss_box_reg: 0.271  loss_mask: 0.1735  loss_rpn_cls: 0.03131  loss_rpn_loc: 0.1203    time: 0.7166  last_time: 0.9492  data_time: 0.5087  last_data_time: 0.6745   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:32:40 d2.utils.events]: \u001b[0m eta: 0:02:33  iter: 1619  total_loss: 0.7647  loss_cls: 0.214  loss_box_reg: 0.2593  loss_mask: 0.168  loss_rpn_cls: 0.02982  loss_rpn_loc: 0.08529    time: 0.7144  last_time: 0.5515  data_time: 0.3122  last_data_time: 0.3576   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:32:50 d2.utils.events]: \u001b[0m eta: 0:02:25  iter: 1639  total_loss: 0.8046  loss_cls: 0.2036  loss_box_reg: 0.2884  loss_mask: 0.1628  loss_rpn_cls: 0.03121  loss_rpn_loc: 0.1073    time: 0.7122  last_time: 0.5866  data_time: 0.3060  last_data_time: 0.3782   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:33:03 d2.utils.events]: \u001b[0m eta: 0:02:20  iter: 1659  total_loss: 0.7499  loss_cls: 0.1977  loss_box_reg: 0.2809  loss_mask: 0.1597  loss_rpn_cls: 0.01381  loss_rpn_loc: 0.1043    time: 0.7114  last_time: 0.2584  data_time: 0.4096  last_data_time: 0.0091   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:33:17 d2.utils.events]: \u001b[0m eta: 0:02:14  iter: 1679  total_loss: 0.8535  loss_cls: 0.2219  loss_box_reg: 0.2708  loss_mask: 0.1681  loss_rpn_cls: 0.02968  loss_rpn_loc: 0.1097    time: 0.7113  last_time: 0.3444  data_time: 0.4554  last_data_time: 0.1264   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:33:31 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:33:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:33:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:33:31 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:33:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:33:31 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:33:31 d2.utils.events]: \u001b[0m eta: 0:01:58  iter: 1699  total_loss: 0.8167  loss_cls: 0.233  loss_box_reg: 0.2927  loss_mask: 0.1477  loss_rpn_cls: 0.03768  loss_rpn_loc: 0.1419    time: 0.7112  last_time: 1.3921  data_time: 0.4722  last_data_time: 1.1592   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:33:44 d2.utils.events]: \u001b[0m eta: 0:01:52  iter: 1719  total_loss: 0.9301  loss_cls: 0.2132  loss_box_reg: 0.3025  loss_mask: 0.1729  loss_rpn_cls: 0.04301  loss_rpn_loc: 0.1263    time: 0.7105  last_time: 0.6636  data_time: 0.4337  last_data_time: 0.4669   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:33:57 d2.utils.events]: \u001b[0m eta: 0:01:42  iter: 1739  total_loss: 0.7292  loss_cls: 0.1867  loss_box_reg: 0.243  loss_mask: 0.1725  loss_rpn_cls: 0.02608  loss_rpn_loc: 0.1006    time: 0.7093  last_time: 0.4383  data_time: 0.3693  last_data_time: 0.2574   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:34:08 d2.utils.events]: \u001b[0m eta: 0:01:32  iter: 1759  total_loss: 0.6318  loss_cls: 0.1623  loss_box_reg: 0.1986  loss_mask: 0.1242  loss_rpn_cls: 0.01436  loss_rpn_loc: 0.1035    time: 0.7079  last_time: 0.2561  data_time: 0.3614  last_data_time: 0.0099   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:34:21 d2.utils.events]: \u001b[0m eta: 0:01:25  iter: 1779  total_loss: 0.8094  loss_cls: 0.1934  loss_box_reg: 0.2438  loss_mask: 0.1646  loss_rpn_cls: 0.02718  loss_rpn_loc: 0.1275    time: 0.7069  last_time: 0.3995  data_time: 0.3800  last_data_time: 0.1663   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:34:34 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:34:34 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:34:34 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:34:34 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:34:34 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:34:34 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:34:34 d2.utils.events]: \u001b[0m eta: 0:01:22  iter: 1799  total_loss: 0.7786  loss_cls: 0.2163  loss_box_reg: 0.2586  loss_mask: 0.1633  loss_rpn_cls: 0.01949  loss_rpn_loc: 0.1057    time: 0.7065  last_time: 0.6313  data_time: 0.4485  last_data_time: 0.4271   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:34:46 d2.utils.events]: \u001b[0m eta: 0:01:12  iter: 1819  total_loss: 0.6677  loss_cls: 0.1599  loss_box_reg: 0.2369  loss_mask: 0.1437  loss_rpn_cls: 0.0309  loss_rpn_loc: 0.1024    time: 0.7053  last_time: 0.2680  data_time: 0.3752  last_data_time: 0.0120   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:35:00 d2.utils.events]: \u001b[0m eta: 0:01:04  iter: 1839  total_loss: 0.7636  loss_cls: 0.2544  loss_box_reg: 0.2264  loss_mask: 0.1597  loss_rpn_cls: 0.02822  loss_rpn_loc: 0.1016    time: 0.7049  last_time: 1.2675  data_time: 0.4433  last_data_time: 1.0404   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:35:17 d2.utils.events]: \u001b[0m eta: 0:00:55  iter: 1859  total_loss: 0.8844  loss_cls: 0.2463  loss_box_reg: 0.2909  loss_mask: 0.1674  loss_rpn_cls: 0.03045  loss_rpn_loc: 0.1195    time: 0.7066  last_time: 0.3437  data_time: 0.5325  last_data_time: 0.0077   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:35:31 d2.utils.events]: \u001b[0m eta: 0:00:46  iter: 1879  total_loss: 0.8219  loss_cls: 0.2059  loss_box_reg: 0.2471  loss_mask: 0.1525  loss_rpn_cls: 0.01879  loss_rpn_loc: 0.1089    time: 0.7069  last_time: 0.3715  data_time: 0.4215  last_data_time: 0.1254   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:35:47 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:35:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:35:47 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:35:47 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:35:47 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:35:47 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/18 16:35:47 d2.utils.events]: \u001b[0m eta: 0:00:38  iter: 1899  total_loss: 0.7171  loss_cls: 0.1947  loss_box_reg: 0.263  loss_mask: 0.1541  loss_rpn_cls: 0.03849  loss_rpn_loc: 0.1026    time: 0.7076  last_time: 0.2398  data_time: 0.4957  last_data_time: 0.0069   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:01 d2.utils.events]: \u001b[0m eta: 0:00:30  iter: 1919  total_loss: 0.7543  loss_cls: 0.1828  loss_box_reg: 0.288  loss_mask: 0.1662  loss_rpn_cls: 0.02846  loss_rpn_loc: 0.1027    time: 0.7076  last_time: 0.2796  data_time: 0.4339  last_data_time: 0.0036   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:16 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 1939  total_loss: 0.7174  loss_cls: 0.1888  loss_box_reg: 0.2453  loss_mask: 0.1727  loss_rpn_cls: 0.02125  loss_rpn_loc: 0.09486    time: 0.7077  last_time: 1.8339  data_time: 0.4540  last_data_time: 1.5097   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:28 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 1959  total_loss: 0.7411  loss_cls: 0.2139  loss_box_reg: 0.2441  loss_mask: 0.149  loss_rpn_cls: 0.02311  loss_rpn_loc: 0.121    time: 0.7070  last_time: 1.1708  data_time: 0.3615  last_data_time: 0.8976   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:39 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 1979  total_loss: 0.7111  loss_cls: 0.1624  loss_box_reg: 0.2337  loss_mask: 0.1451  loss_rpn_cls: 0.01491  loss_rpn_loc: 0.1074    time: 0.7054  last_time: 0.5374  data_time: 0.2569  last_data_time: 0.2441   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:57 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 1999  total_loss: 0.7791  loss_cls: 0.1628  loss_box_reg: 0.2626  loss_mask: 0.1735  loss_rpn_cls: 0.02349  loss_rpn_loc: 0.1174    time: 0.7062  last_time: 1.7083  data_time: 0.4783  last_data_time: 1.4151   lr: 0.001  max_mem: 4379M\n",
      "\u001b[32m[07/18 16:36:57 d2.engine.hooks]: \u001b[0mOverall training speed: 1998 iterations in 0:23:31 (0.7062 s / it)\n",
      "\u001b[32m[07/18 16:36:57 d2.engine.hooks]: \u001b[0mTotal training time: 0:23:35 (0:00:04 on hooks)\n",
      "\u001b[32m[07/18 16:36:57 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/18 16:36:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/18 16:36:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/18 16:36:57 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/18 16:36:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/18 16:36:57 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/19 13:03:02 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[07/19 13:03:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip(), <detectron2.data.transforms.augmentation_impl.RandomApply object at 0x000001F7E753D050>, <detectron2.data.transforms.augmentation_impl.RandomApply object at 0x000001F7E88CEF50>, <detectron2.data.transforms.augmentation_impl.RandomApply object at 0x000001F780AF2A50>, <detectron2.data.transforms.augmentation_impl.RandomApply object at 0x000001F78C634C90>]\n",
      "\u001b[32m[07/19 13:03:03 d2.data.datasets.coco]: \u001b[0mLoaded 109 images in COCO format from datasets/train/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:03:03 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 109 images left.\n",
      "\u001b[32m[07/19 13:03:03 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    bloc    | 8841         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/19 13:03:03 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/19 13:03:03 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:03:03 d2.data.common]: \u001b[0mSerializing 109 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:03:03 d2.data.common]: \u001b[0mSerialized dataset takes 3.25 MiB\n",
      "\u001b[32m[07/19 13:03:03 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:03:03 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[07/19 13:03:03 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from https://dl.fbaipublicfiles.com/detectron2/COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x/138205316/model_final_a3ec72.pkl ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.weight' to the model due to incompatible shapes: (80, 256, 1, 1) in the checkpoint but (1, 256, 1, 1) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.mask_head.predictor.bias' to the model due to incompatible shapes: (80,) in the checkpoint but (1,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/19 13:03:03 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\myenv\\Lib\\site-packages\\torch\\functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3588.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/19 13:03:25 d2.utils.events]: \u001b[0m eta: 0:09:21  iter: 19  total_loss: 6.026  loss_cls: 0.6295  loss_box_reg: 0.4684  loss_mask: 0.6968  loss_rpn_cls: 3.982  loss_rpn_loc: 0.2776    time: 0.8257  last_time: 0.2116  data_time: 0.7822  last_data_time: 0.0165   lr: 1.9981e-05  max_mem: 3594M\n",
      "\u001b[32m[07/19 13:03:39 d2.utils.events]: \u001b[0m eta: 0:09:17  iter: 39  total_loss: 2.766  loss_cls: 0.6103  loss_box_reg: 0.679  loss_mask: 0.6673  loss_rpn_cls: 0.6187  loss_rpn_loc: 0.2058    time: 0.7568  last_time: 0.2463  data_time: 0.4681  last_data_time: 0.0201   lr: 3.9961e-05  max_mem: 3594M\n",
      "\u001b[32m[07/19 13:03:56 d2.utils.events]: \u001b[0m eta: 0:09:15  iter: 59  total_loss: 2.172  loss_cls: 0.5564  loss_box_reg: 0.6598  loss_mask: 0.6239  loss_rpn_cls: 0.195  loss_rpn_loc: 0.1637    time: 0.7917  last_time: 0.2682  data_time: 0.6015  last_data_time: 0.0066   lr: 5.9941e-05  max_mem: 4057M\n",
      "\u001b[32m[07/19 13:04:12 d2.utils.events]: \u001b[0m eta: 0:09:18  iter: 79  total_loss: 2.011  loss_cls: 0.5043  loss_box_reg: 0.6709  loss_mask: 0.5436  loss_rpn_cls: 0.1661  loss_rpn_loc: 0.1478    time: 0.7868  last_time: 1.0008  data_time: 0.5392  last_data_time: 0.7476   lr: 7.9921e-05  max_mem: 4057M\n",
      "\u001b[32m[07/19 13:04:26 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:04:26 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|    bloc    | 2107         |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[07/19 13:04:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:04:26 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:04:26 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:04:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:04:26 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:04:26 d2.utils.events]: \u001b[0m eta: 0:14:18  iter: 99  total_loss: 1.863  loss_cls: 0.4775  loss_box_reg: 0.565  loss_mask: 0.4741  loss_rpn_cls: 0.1633  loss_rpn_loc: 0.1511    time: 0.7724  last_time: 0.1928  data_time: 0.4837  last_data_time: 0.0080   lr: 9.9901e-05  max_mem: 4057M\n",
      "\u001b[32m[07/19 13:04:42 d2.utils.events]: \u001b[0m eta: 0:14:49  iter: 119  total_loss: 2.034  loss_cls: 0.4678  loss_box_reg: 0.7691  loss_mask: 0.4454  loss_rpn_cls: 0.1345  loss_rpn_loc: 0.1851    time: 0.7765  last_time: 1.2868  data_time: 0.5588  last_data_time: 1.0971   lr: 0.00011988  max_mem: 4057M\n",
      "\u001b[32m[07/19 13:04:55 d2.utils.events]: \u001b[0m eta: 0:15:18  iter: 139  total_loss: 1.8  loss_cls: 0.4274  loss_box_reg: 0.6554  loss_mask: 0.3664  loss_rpn_cls: 0.1274  loss_rpn_loc: 0.1708    time: 0.7597  last_time: 1.0786  data_time: 0.4318  last_data_time: 0.8229   lr: 0.00013986  max_mem: 4057M\n",
      "\u001b[32m[07/19 13:05:11 d2.utils.events]: \u001b[0m eta: 0:14:30  iter: 159  total_loss: 1.589  loss_cls: 0.4039  loss_box_reg: 0.6282  loss_mask: 0.3208  loss_rpn_cls: 0.1068  loss_rpn_loc: 0.1422    time: 0.7630  last_time: 0.1963  data_time: 0.5133  last_data_time: 0.0014   lr: 0.00015984  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:05:25 d2.utils.events]: \u001b[0m eta: 0:13:51  iter: 179  total_loss: 1.534  loss_cls: 0.4004  loss_box_reg: 0.5351  loss_mask: 0.2951  loss_rpn_cls: 0.1248  loss_rpn_loc: 0.1347    time: 0.7580  last_time: 1.5490  data_time: 0.4900  last_data_time: 1.3496   lr: 0.00017982  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:05:39 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:05:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:05:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:05:39 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:05:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:05:39 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:05:39 d2.utils.events]: \u001b[0m eta: 0:13:58  iter: 199  total_loss: 1.52  loss_cls: 0.3838  loss_box_reg: 0.5714  loss_mask: 0.2679  loss_rpn_cls: 0.1086  loss_rpn_loc: 0.1676    time: 0.7513  last_time: 0.2197  data_time: 0.4545  last_data_time: 0.0145   lr: 0.0001998  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:05:57 d2.utils.events]: \u001b[0m eta: 0:13:33  iter: 219  total_loss: 1.512  loss_cls: 0.3701  loss_box_reg: 0.5656  loss_mask: 0.2566  loss_rpn_cls: 0.1509  loss_rpn_loc: 0.1558    time: 0.7609  last_time: 0.3976  data_time: 0.5965  last_data_time: 0.1340   lr: 0.00021978  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:06:12 d2.utils.events]: \u001b[0m eta: 0:13:23  iter: 239  total_loss: 1.353  loss_cls: 0.3242  loss_box_reg: 0.5596  loss_mask: 0.2658  loss_rpn_cls: 0.1189  loss_rpn_loc: 0.1472    time: 0.7604  last_time: 0.9801  data_time: 0.5168  last_data_time: 0.8118   lr: 0.00023976  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:06:23 d2.utils.events]: \u001b[0m eta: 0:13:14  iter: 259  total_loss: 1.231  loss_cls: 0.3278  loss_box_reg: 0.467  loss_mask: 0.228  loss_rpn_cls: 0.1144  loss_rpn_loc: 0.1543    time: 0.7468  last_time: 0.2336  data_time: 0.3549  last_data_time: 0.0068   lr: 0.00025974  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:06:39 d2.utils.events]: \u001b[0m eta: 0:13:05  iter: 279  total_loss: 1.109  loss_cls: 0.2861  loss_box_reg: 0.378  loss_mask: 0.2339  loss_rpn_cls: 0.07056  loss_rpn_loc: 0.1338    time: 0.7495  last_time: 0.2368  data_time: 0.5604  last_data_time: 0.0117   lr: 0.00027972  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:06:54 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:06:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:06:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:06:54 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:06:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:06:54 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:06:54 d2.utils.events]: \u001b[0m eta: 0:12:56  iter: 299  total_loss: 1.134  loss_cls: 0.2973  loss_box_reg: 0.3801  loss_mask: 0.2188  loss_rpn_cls: 0.1065  loss_rpn_loc: 0.1296    time: 0.7496  last_time: 1.0948  data_time: 0.4869  last_data_time: 0.8754   lr: 0.0002997  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:07:10 d2.utils.events]: \u001b[0m eta: 0:12:47  iter: 319  total_loss: 1.21  loss_cls: 0.3377  loss_box_reg: 0.4304  loss_mask: 0.2175  loss_rpn_cls: 0.1132  loss_rpn_loc: 0.1495    time: 0.7511  last_time: 0.2322  data_time: 0.5211  last_data_time: 0.0083   lr: 0.00031968  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:07:24 d2.utils.events]: \u001b[0m eta: 0:12:24  iter: 339  total_loss: 0.9841  loss_cls: 0.2722  loss_box_reg: 0.3359  loss_mask: 0.2004  loss_rpn_cls: 0.0869  loss_rpn_loc: 0.1135    time: 0.7499  last_time: 0.2092  data_time: 0.5016  last_data_time: 0.0104   lr: 0.00033966  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:07:39 d2.utils.events]: \u001b[0m eta: 0:12:29  iter: 359  total_loss: 1.117  loss_cls: 0.2904  loss_box_reg: 0.3689  loss_mask: 0.2047  loss_rpn_cls: 0.07149  loss_rpn_loc: 0.1329    time: 0.7488  last_time: 1.3491  data_time: 0.5147  last_data_time: 1.1402   lr: 0.00035964  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:07:52 d2.utils.events]: \u001b[0m eta: 0:12:06  iter: 379  total_loss: 1.143  loss_cls: 0.3451  loss_box_reg: 0.3394  loss_mask: 0.2136  loss_rpn_cls: 0.08271  loss_rpn_loc: 0.1538    time: 0.7445  last_time: 0.2465  data_time: 0.4373  last_data_time: 0.0032   lr: 0.00037962  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:08:07 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:08:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:08:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:08:07 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:08:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:08:07 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:08:07 d2.utils.events]: \u001b[0m eta: 0:11:57  iter: 399  total_loss: 1.168  loss_cls: 0.3225  loss_box_reg: 0.342  loss_mask: 0.2268  loss_rpn_cls: 0.1085  loss_rpn_loc: 0.142    time: 0.7442  last_time: 0.9034  data_time: 0.5188  last_data_time: 0.7022   lr: 0.0003996  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:08:21 d2.utils.events]: \u001b[0m eta: 0:12:01  iter: 419  total_loss: 1.242  loss_cls: 0.3416  loss_box_reg: 0.3602  loss_mask: 0.2308  loss_rpn_cls: 0.09693  loss_rpn_loc: 0.1668    time: 0.7423  last_time: 0.7564  data_time: 0.4741  last_data_time: 0.5594   lr: 0.00041958  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:08:36 d2.utils.events]: \u001b[0m eta: 0:11:29  iter: 439  total_loss: 1.178  loss_cls: 0.3272  loss_box_reg: 0.3915  loss_mask: 0.2217  loss_rpn_cls: 0.07629  loss_rpn_loc: 0.143    time: 0.7411  last_time: 0.2098  data_time: 0.4735  last_data_time: 0.0077   lr: 0.00043956  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:08:50 d2.utils.events]: \u001b[0m eta: 0:11:20  iter: 459  total_loss: 1.024  loss_cls: 0.3211  loss_box_reg: 0.3196  loss_mask: 0.1966  loss_rpn_cls: 0.07566  loss_rpn_loc: 0.1384    time: 0.7395  last_time: 0.2277  data_time: 0.4701  last_data_time: 0.0354   lr: 0.00045954  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:09:04 d2.utils.events]: \u001b[0m eta: 0:11:14  iter: 479  total_loss: 1.032  loss_cls: 0.2676  loss_box_reg: 0.3171  loss_mask: 0.211  loss_rpn_cls: 0.121  loss_rpn_loc: 0.1324    time: 0.7387  last_time: 0.2088  data_time: 0.4934  last_data_time: 0.0093   lr: 0.00047952  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:09:17 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:09:17 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:09:17 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:09:17 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:09:17 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:09:17 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:09:17 d2.utils.events]: \u001b[0m eta: 0:11:02  iter: 499  total_loss: 1.054  loss_cls: 0.2829  loss_box_reg: 0.3111  loss_mask: 0.1943  loss_rpn_cls: 0.0945  loss_rpn_loc: 0.1197    time: 0.7351  last_time: 0.4908  data_time: 0.4283  last_data_time: 0.2735   lr: 0.0004995  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:09:32 d2.utils.events]: \u001b[0m eta: 0:10:56  iter: 519  total_loss: 1.004  loss_cls: 0.2937  loss_box_reg: 0.3131  loss_mask: 0.2079  loss_rpn_cls: 0.07348  loss_rpn_loc: 0.1123    time: 0.7345  last_time: 0.5842  data_time: 0.5003  last_data_time: 0.3829   lr: 0.00051948  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:09:46 d2.utils.events]: \u001b[0m eta: 0:10:44  iter: 539  total_loss: 1.085  loss_cls: 0.2637  loss_box_reg: 0.3301  loss_mask: 0.2074  loss_rpn_cls: 0.1176  loss_rpn_loc: 0.139    time: 0.7337  last_time: 0.8831  data_time: 0.4849  last_data_time: 0.6933   lr: 0.00053946  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:10:01 d2.utils.events]: \u001b[0m eta: 0:10:36  iter: 559  total_loss: 0.9629  loss_cls: 0.266  loss_box_reg: 0.304  loss_mask: 0.1788  loss_rpn_cls: 0.09231  loss_rpn_loc: 0.12    time: 0.7342  last_time: 0.3340  data_time: 0.5148  last_data_time: 0.1430   lr: 0.00055944  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:10:17 d2.utils.events]: \u001b[0m eta: 0:10:48  iter: 579  total_loss: 0.9833  loss_cls: 0.3145  loss_box_reg: 0.3198  loss_mask: 0.2024  loss_rpn_cls: 0.06367  loss_rpn_loc: 0.1176    time: 0.7361  last_time: 1.9113  data_time: 0.5604  last_data_time: 1.6192   lr: 0.00057942  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:10:32 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:10:32 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:10:32 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:10:32 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:10:32 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:10:32 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:10:32 d2.utils.events]: \u001b[0m eta: 0:10:39  iter: 599  total_loss: 1.028  loss_cls: 0.3017  loss_box_reg: 0.3524  loss_mask: 0.2038  loss_rpn_cls: 0.1001  loss_rpn_loc: 0.1226    time: 0.7380  last_time: 0.2579  data_time: 0.5462  last_data_time: 0.0089   lr: 0.0005994  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:10:48 d2.utils.events]: \u001b[0m eta: 0:10:18  iter: 619  total_loss: 1.142  loss_cls: 0.3058  loss_box_reg: 0.3581  loss_mask: 0.2123  loss_rpn_cls: 0.05323  loss_rpn_loc: 0.1272    time: 0.7386  last_time: 0.8507  data_time: 0.5474  last_data_time: 0.6551   lr: 0.00061938  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:10:59 d2.utils.events]: \u001b[0m eta: 0:10:21  iter: 639  total_loss: 0.9475  loss_cls: 0.241  loss_box_reg: 0.3023  loss_mask: 0.197  loss_rpn_cls: 0.06642  loss_rpn_loc: 0.1375    time: 0.7334  last_time: 0.6081  data_time: 0.3463  last_data_time: 0.4357   lr: 0.00063936  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:11:11 d2.utils.events]: \u001b[0m eta: 0:10:12  iter: 659  total_loss: 0.9056  loss_cls: 0.2332  loss_box_reg: 0.3013  loss_mask: 0.1922  loss_rpn_cls: 0.06268  loss_rpn_loc: 0.1365    time: 0.7290  last_time: 0.2321  data_time: 0.3668  last_data_time: 0.0079   lr: 0.00065934  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:11:26 d2.utils.events]: \u001b[0m eta: 0:10:07  iter: 679  total_loss: 0.9385  loss_cls: 0.2487  loss_box_reg: 0.2934  loss_mask: 0.1787  loss_rpn_cls: 0.05038  loss_rpn_loc: 0.1212    time: 0.7298  last_time: 0.2692  data_time: 0.5250  last_data_time: 0.0031   lr: 0.00067932  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:11:41 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:11:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:11:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:11:41 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:11:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:11:41 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:11:41 d2.utils.events]: \u001b[0m eta: 0:09:53  iter: 699  total_loss: 1.094  loss_cls: 0.3298  loss_box_reg: 0.3299  loss_mask: 0.194  loss_rpn_cls: 0.05599  loss_rpn_loc: 0.1136    time: 0.7310  last_time: 1.3264  data_time: 0.5347  last_data_time: 1.1450   lr: 0.0006993  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:11:57 d2.utils.events]: \u001b[0m eta: 0:09:44  iter: 719  total_loss: 0.906  loss_cls: 0.2712  loss_box_reg: 0.2841  loss_mask: 0.2016  loss_rpn_cls: 0.06091  loss_rpn_loc: 0.1213    time: 0.7317  last_time: 0.6363  data_time: 0.5108  last_data_time: 0.4154   lr: 0.00071928  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:12:12 d2.utils.events]: \u001b[0m eta: 0:09:40  iter: 739  total_loss: 1.022  loss_cls: 0.2849  loss_box_reg: 0.3328  loss_mask: 0.2143  loss_rpn_cls: 0.06457  loss_rpn_loc: 0.1381    time: 0.7325  last_time: 1.6559  data_time: 0.5157  last_data_time: 1.4594   lr: 0.00073926  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:12:25 d2.utils.events]: \u001b[0m eta: 0:09:31  iter: 759  total_loss: 1.084  loss_cls: 0.293  loss_box_reg: 0.3413  loss_mask: 0.2053  loss_rpn_cls: 0.07633  loss_rpn_loc: 0.1428    time: 0.7301  last_time: 0.9053  data_time: 0.4122  last_data_time: 0.6765   lr: 0.00075924  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:12:39 d2.utils.events]: \u001b[0m eta: 0:09:21  iter: 779  total_loss: 0.9129  loss_cls: 0.2636  loss_box_reg: 0.3375  loss_mask: 0.1971  loss_rpn_cls: 0.05108  loss_rpn_loc: 0.1385    time: 0.7302  last_time: 1.5932  data_time: 0.5051  last_data_time: 1.3166   lr: 0.00077922  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:12:52 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:12:52 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:12:52 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:12:52 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:12:52 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:12:52 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:12:52 d2.utils.events]: \u001b[0m eta: 0:08:52  iter: 799  total_loss: 1.076  loss_cls: 0.2972  loss_box_reg: 0.3012  loss_mask: 0.2086  loss_rpn_cls: 0.09791  loss_rpn_loc: 0.1392    time: 0.7274  last_time: 0.2201  data_time: 0.3836  last_data_time: 0.0141   lr: 0.0007992  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:13:07 d2.utils.events]: \u001b[0m eta: 0:08:43  iter: 819  total_loss: 1.102  loss_cls: 0.3176  loss_box_reg: 0.3474  loss_mask: 0.2049  loss_rpn_cls: 0.06605  loss_rpn_loc: 0.1345    time: 0.7286  last_time: 0.6764  data_time: 0.5475  last_data_time: 0.4714   lr: 0.00081918  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:13:22 d2.utils.events]: \u001b[0m eta: 0:08:49  iter: 839  total_loss: 0.9355  loss_cls: 0.2682  loss_box_reg: 0.2931  loss_mask: 0.1813  loss_rpn_cls: 0.06254  loss_rpn_loc: 0.123    time: 0.7283  last_time: 0.2178  data_time: 0.4925  last_data_time: 0.0130   lr: 0.00083916  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:13:39 d2.utils.events]: \u001b[0m eta: 0:08:44  iter: 859  total_loss: 0.852  loss_cls: 0.2368  loss_box_reg: 0.2845  loss_mask: 0.1669  loss_rpn_cls: 0.06653  loss_rpn_loc: 0.1233    time: 0.7316  last_time: 0.9120  data_time: 0.6230  last_data_time: 0.6948   lr: 0.00085914  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:13:52 d2.utils.events]: \u001b[0m eta: 0:08:31  iter: 879  total_loss: 0.9468  loss_cls: 0.2782  loss_box_reg: 0.3029  loss_mask: 0.1818  loss_rpn_cls: 0.05072  loss_rpn_loc: 0.1245    time: 0.7292  last_time: 0.2514  data_time: 0.4050  last_data_time: 0.0102   lr: 0.00087912  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:14:06 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:14:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:14:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:14:06 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:14:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:14:06 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:14:06 d2.utils.events]: \u001b[0m eta: 0:08:21  iter: 899  total_loss: 0.899  loss_cls: 0.2413  loss_box_reg: 0.308  loss_mask: 0.1789  loss_rpn_cls: 0.04793  loss_rpn_loc: 0.1232    time: 0.7289  last_time: 0.6403  data_time: 0.4802  last_data_time: 0.3800   lr: 0.0008991  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:14:20 d2.utils.events]: \u001b[0m eta: 0:08:15  iter: 919  total_loss: 1.126  loss_cls: 0.3371  loss_box_reg: 0.3687  loss_mask: 0.2007  loss_rpn_cls: 0.05518  loss_rpn_loc: 0.1446    time: 0.7278  last_time: 1.3222  data_time: 0.4597  last_data_time: 1.0922   lr: 0.00091908  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:14:32 d2.utils.events]: \u001b[0m eta: 0:07:55  iter: 939  total_loss: 0.8922  loss_cls: 0.2434  loss_box_reg: 0.3177  loss_mask: 0.1771  loss_rpn_cls: 0.05686  loss_rpn_loc: 0.1136    time: 0.7255  last_time: 0.2439  data_time: 0.3820  last_data_time: 0.0100   lr: 0.00093906  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:14:47 d2.utils.events]: \u001b[0m eta: 0:07:41  iter: 959  total_loss: 0.8796  loss_cls: 0.231  loss_box_reg: 0.308  loss_mask: 0.1592  loss_rpn_cls: 0.06396  loss_rpn_loc: 0.1295    time: 0.7263  last_time: 0.2173  data_time: 0.5318  last_data_time: 0.0121   lr: 0.00095904  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:15:03 d2.utils.events]: \u001b[0m eta: 0:07:32  iter: 979  total_loss: 0.9361  loss_cls: 0.2378  loss_box_reg: 0.3313  loss_mask: 0.1852  loss_rpn_cls: 0.05405  loss_rpn_loc: 0.1261    time: 0.7271  last_time: 0.2302  data_time: 0.5355  last_data_time: 0.0154   lr: 0.00097902  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:15:16 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:15:16 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:15:16 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:15:16 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:15:16 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:15:16 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:15:16 d2.utils.events]: \u001b[0m eta: 0:07:23  iter: 999  total_loss: 0.9656  loss_cls: 0.2792  loss_box_reg: 0.3051  loss_mask: 0.1978  loss_rpn_cls: 0.05798  loss_rpn_loc: 0.1263    time: 0.7252  last_time: 0.2458  data_time: 0.4050  last_data_time: 0.0078   lr: 0.000999  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:15:30 d2.utils.events]: \u001b[0m eta: 0:07:17  iter: 1019  total_loss: 0.8737  loss_cls: 0.2038  loss_box_reg: 0.3045  loss_mask: 0.1962  loss_rpn_cls: 0.0546  loss_rpn_loc: 0.1082    time: 0.7247  last_time: 1.0876  data_time: 0.4795  last_data_time: 0.8495   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:15:45 d2.utils.events]: \u001b[0m eta: 0:07:05  iter: 1039  total_loss: 0.9015  loss_cls: 0.2504  loss_box_reg: 0.2749  loss_mask: 0.1804  loss_rpn_cls: 0.06563  loss_rpn_loc: 0.1428    time: 0.7254  last_time: 0.1911  data_time: 0.5045  last_data_time: 0.0071   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:16:00 d2.utils.events]: \u001b[0m eta: 0:06:51  iter: 1059  total_loss: 0.9038  loss_cls: 0.2439  loss_box_reg: 0.285  loss_mask: 0.1872  loss_rpn_cls: 0.05177  loss_rpn_loc: 0.1206    time: 0.7260  last_time: 0.2568  data_time: 0.5323  last_data_time: 0.0157   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:16:15 d2.utils.events]: \u001b[0m eta: 0:06:40  iter: 1079  total_loss: 0.8943  loss_cls: 0.239  loss_box_reg: 0.2891  loss_mask: 0.189  loss_rpn_cls: 0.05344  loss_rpn_loc: 0.1425    time: 0.7257  last_time: 1.7681  data_time: 0.4787  last_data_time: 1.4946   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:16:28 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:16:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:16:28 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:16:28 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:16:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:16:28 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:16:28 d2.utils.events]: \u001b[0m eta: 0:06:27  iter: 1099  total_loss: 0.9174  loss_cls: 0.2715  loss_box_reg: 0.3085  loss_mask: 0.192  loss_rpn_cls: 0.08062  loss_rpn_loc: 0.1397    time: 0.7248  last_time: 0.7897  data_time: 0.4603  last_data_time: 0.5655   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:16:39 d2.utils.events]: \u001b[0m eta: 0:06:17  iter: 1119  total_loss: 0.7586  loss_cls: 0.2137  loss_box_reg: 0.255  loss_mask: 0.1354  loss_rpn_cls: 0.04306  loss_rpn_loc: 0.1391    time: 0.7213  last_time: 0.2442  data_time: 0.3109  last_data_time: 0.0117   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:16:57 d2.utils.events]: \u001b[0m eta: 0:06:09  iter: 1139  total_loss: 0.9804  loss_cls: 0.2335  loss_box_reg: 0.2899  loss_mask: 0.1721  loss_rpn_cls: 0.05783  loss_rpn_loc: 0.1392    time: 0.7246  last_time: 0.2461  data_time: 0.6819  last_data_time: 0.0360   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:17:11 d2.utils.events]: \u001b[0m eta: 0:05:58  iter: 1159  total_loss: 0.9974  loss_cls: 0.2607  loss_box_reg: 0.3095  loss_mask: 0.2054  loss_rpn_cls: 0.06504  loss_rpn_loc: 0.1255    time: 0.7243  last_time: 1.0300  data_time: 0.4729  last_data_time: 0.7645   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:17:26 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 1179  total_loss: 1.025  loss_cls: 0.2645  loss_box_reg: 0.3131  loss_mask: 0.2225  loss_rpn_cls: 0.07173  loss_rpn_loc: 0.1342    time: 0.7243  last_time: 0.3868  data_time: 0.4988  last_data_time: 0.1998   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:17:39 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:17:39 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:17:39 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:17:39 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:17:39 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:17:39 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:17:39 d2.utils.events]: \u001b[0m eta: 0:05:43  iter: 1199  total_loss: 0.9137  loss_cls: 0.2597  loss_box_reg: 0.2978  loss_mask: 0.2024  loss_rpn_cls: 0.05589  loss_rpn_loc: 0.1432    time: 0.7236  last_time: 0.3998  data_time: 0.4676  last_data_time: 0.1653   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:17:54 d2.utils.events]: \u001b[0m eta: 0:05:34  iter: 1219  total_loss: 1.073  loss_cls: 0.3102  loss_box_reg: 0.3512  loss_mask: 0.2025  loss_rpn_cls: 0.06817  loss_rpn_loc: 0.1358    time: 0.7241  last_time: 0.2233  data_time: 0.5097  last_data_time: 0.0123   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:18:08 d2.utils.events]: \u001b[0m eta: 0:05:26  iter: 1239  total_loss: 0.782  loss_cls: 0.2449  loss_box_reg: 0.2567  loss_mask: 0.1581  loss_rpn_cls: 0.03774  loss_rpn_loc: 0.1155    time: 0.7231  last_time: 0.1859  data_time: 0.4399  last_data_time: 0.0075   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:18:24 d2.utils.events]: \u001b[0m eta: 0:05:17  iter: 1259  total_loss: 0.8749  loss_cls: 0.2613  loss_box_reg: 0.3028  loss_mask: 0.1829  loss_rpn_cls: 0.0286  loss_rpn_loc: 0.1184    time: 0.7244  last_time: 1.7827  data_time: 0.5640  last_data_time: 1.5447   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:18:38 d2.utils.events]: \u001b[0m eta: 0:05:07  iter: 1279  total_loss: 0.8272  loss_cls: 0.257  loss_box_reg: 0.2807  loss_mask: 0.177  loss_rpn_cls: 0.03694  loss_rpn_loc: 0.1294    time: 0.7245  last_time: 0.2878  data_time: 0.4872  last_data_time: 0.0530   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:18:53 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:18:53 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:18:53 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:18:53 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:18:53 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:18:53 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:18:53 d2.utils.events]: \u001b[0m eta: 0:04:54  iter: 1299  total_loss: 0.8095  loss_cls: 0.218  loss_box_reg: 0.24  loss_mask: 0.1616  loss_rpn_cls: 0.04822  loss_rpn_loc: 0.1245    time: 0.7243  last_time: 0.2148  data_time: 0.4813  last_data_time: 0.0021   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:19:09 d2.utils.events]: \u001b[0m eta: 0:04:48  iter: 1319  total_loss: 0.9744  loss_cls: 0.2292  loss_box_reg: 0.3359  loss_mask: 0.1991  loss_rpn_cls: 0.04917  loss_rpn_loc: 0.1098    time: 0.7257  last_time: 2.0202  data_time: 0.5805  last_data_time: 1.7172   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:19:21 d2.utils.events]: \u001b[0m eta: 0:04:39  iter: 1339  total_loss: 0.8273  loss_cls: 0.2075  loss_box_reg: 0.2826  loss_mask: 0.1491  loss_rpn_cls: 0.05136  loss_rpn_loc: 0.1345    time: 0.7234  last_time: 1.0406  data_time: 0.3488  last_data_time: 0.7752   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:19:37 d2.utils.events]: \u001b[0m eta: 0:04:26  iter: 1359  total_loss: 0.8588  loss_cls: 0.2125  loss_box_reg: 0.3307  loss_mask: 0.1719  loss_rpn_cls: 0.04533  loss_rpn_loc: 0.1142    time: 0.7250  last_time: 1.7767  data_time: 0.6017  last_data_time: 1.5335   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:19:52 d2.utils.events]: \u001b[0m eta: 0:04:22  iter: 1379  total_loss: 0.947  loss_cls: 0.2488  loss_box_reg: 0.2572  loss_mask: 0.1658  loss_rpn_cls: 0.09997  loss_rpn_loc: 0.1596    time: 0.7253  last_time: 1.0784  data_time: 0.5139  last_data_time: 0.8723   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:20:06 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:20:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:20:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:20:06 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:20:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:20:06 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:20:06 d2.utils.events]: \u001b[0m eta: 0:04:13  iter: 1399  total_loss: 0.9975  loss_cls: 0.2668  loss_box_reg: 0.3245  loss_mask: 0.187  loss_rpn_cls: 0.06437  loss_rpn_loc: 0.1303    time: 0.7249  last_time: 0.2635  data_time: 0.4701  last_data_time: 0.0190   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:20:21 d2.utils.events]: \u001b[0m eta: 0:04:01  iter: 1419  total_loss: 0.8261  loss_cls: 0.2247  loss_box_reg: 0.2919  loss_mask: 0.1819  loss_rpn_cls: 0.04561  loss_rpn_loc: 0.1116    time: 0.7254  last_time: 1.6278  data_time: 0.5318  last_data_time: 1.4413   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:20:40 d2.utils.events]: \u001b[0m eta: 0:03:54  iter: 1439  total_loss: 0.7987  loss_cls: 0.1967  loss_box_reg: 0.2544  loss_mask: 0.163  loss_rpn_cls: 0.03044  loss_rpn_loc: 0.1055    time: 0.7283  last_time: 1.3126  data_time: 0.6842  last_data_time: 1.1201   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:20:54 d2.utils.events]: \u001b[0m eta: 0:03:42  iter: 1459  total_loss: 0.7466  loss_cls: 0.2183  loss_box_reg: 0.246  loss_mask: 0.1417  loss_rpn_cls: 0.03323  loss_rpn_loc: 0.145    time: 0.7278  last_time: 1.2812  data_time: 0.4591  last_data_time: 1.0986   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:21:10 d2.utils.events]: \u001b[0m eta: 0:03:27  iter: 1479  total_loss: 1.087  loss_cls: 0.2498  loss_box_reg: 0.343  loss_mask: 0.2209  loss_rpn_cls: 0.05378  loss_rpn_loc: 0.1253    time: 0.7290  last_time: 0.2868  data_time: 0.5643  last_data_time: 0.0080   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:21:23 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:21:23 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:21:23 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:21:23 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:21:23 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:21:23 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:21:23 d2.utils.events]: \u001b[0m eta: 0:03:19  iter: 1499  total_loss: 0.7778  loss_cls: 0.2126  loss_box_reg: 0.2405  loss_mask: 0.1472  loss_rpn_cls: 0.04409  loss_rpn_loc: 0.1057    time: 0.7275  last_time: 0.1919  data_time: 0.3863  last_data_time: 0.0182   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:21:36 d2.utils.events]: \u001b[0m eta: 0:03:09  iter: 1519  total_loss: 0.8444  loss_cls: 0.244  loss_box_reg: 0.271  loss_mask: 0.1731  loss_rpn_cls: 0.03927  loss_rpn_loc: 0.1382    time: 0.7267  last_time: 0.2256  data_time: 0.4493  last_data_time: 0.0102   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:21:50 d2.utils.events]: \u001b[0m eta: 0:03:02  iter: 1539  total_loss: 0.8738  loss_cls: 0.2261  loss_box_reg: 0.3379  loss_mask: 0.1723  loss_rpn_cls: 0.03151  loss_rpn_loc: 0.1184    time: 0.7266  last_time: 0.1987  data_time: 0.4919  last_data_time: 0.0048   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:22:04 d2.utils.events]: \u001b[0m eta: 0:02:54  iter: 1559  total_loss: 0.7935  loss_cls: 0.1886  loss_box_reg: 0.2659  loss_mask: 0.1625  loss_rpn_cls: 0.03716  loss_rpn_loc: 0.1206    time: 0.7263  last_time: 0.9992  data_time: 0.4707  last_data_time: 0.7523   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:22:18 d2.utils.events]: \u001b[0m eta: 0:02:43  iter: 1579  total_loss: 0.9859  loss_cls: 0.2692  loss_box_reg: 0.3249  loss_mask: 0.1945  loss_rpn_cls: 0.03896  loss_rpn_loc: 0.1232    time: 0.7259  last_time: 0.2504  data_time: 0.4660  last_data_time: 0.0080   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:22:33 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:22:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:22:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:22:33 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:22:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:22:33 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:22:33 d2.utils.events]: \u001b[0m eta: 0:02:37  iter: 1599  total_loss: 0.7265  loss_cls: 0.1959  loss_box_reg: 0.2699  loss_mask: 0.1488  loss_rpn_cls: 0.03386  loss_rpn_loc: 0.112    time: 0.7258  last_time: 0.8165  data_time: 0.4909  last_data_time: 0.6236   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:22:46 d2.utils.events]: \u001b[0m eta: 0:02:26  iter: 1619  total_loss: 0.8799  loss_cls: 0.2384  loss_box_reg: 0.2861  loss_mask: 0.1633  loss_rpn_cls: 0.05423  loss_rpn_loc: 0.1251    time: 0.7251  last_time: 0.2276  data_time: 0.4211  last_data_time: 0.0270   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:23:00 d2.utils.events]: \u001b[0m eta: 0:02:18  iter: 1639  total_loss: 0.9903  loss_cls: 0.2399  loss_box_reg: 0.2987  loss_mask: 0.1928  loss_rpn_cls: 0.05644  loss_rpn_loc: 0.1142    time: 0.7249  last_time: 0.7955  data_time: 0.4741  last_data_time: 0.6137   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:23:16 d2.utils.events]: \u001b[0m eta: 0:02:08  iter: 1659  total_loss: 0.8348  loss_cls: 0.2142  loss_box_reg: 0.3016  loss_mask: 0.1835  loss_rpn_cls: 0.03321  loss_rpn_loc: 0.1089    time: 0.7253  last_time: 0.2789  data_time: 0.5345  last_data_time: 0.0237   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:23:33 d2.utils.events]: \u001b[0m eta: 0:01:58  iter: 1679  total_loss: 0.8196  loss_cls: 0.1864  loss_box_reg: 0.2662  loss_mask: 0.1499  loss_rpn_cls: 0.03532  loss_rpn_loc: 0.1207    time: 0.7269  last_time: 0.4926  data_time: 0.6079  last_data_time: 0.3155   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:23:46 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:23:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:23:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:23:46 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:23:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:23:46 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:23:46 d2.utils.events]: \u001b[0m eta: 0:01:52  iter: 1699  total_loss: 0.8819  loss_cls: 0.2077  loss_box_reg: 0.2794  loss_mask: 0.1828  loss_rpn_cls: 0.02844  loss_rpn_loc: 0.1273    time: 0.7259  last_time: 0.1891  data_time: 0.4245  last_data_time: 0.0104   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:24:02 d2.utils.events]: \u001b[0m eta: 0:01:44  iter: 1719  total_loss: 0.8403  loss_cls: 0.2194  loss_box_reg: 0.3019  loss_mask: 0.1679  loss_rpn_cls: 0.04603  loss_rpn_loc: 0.1212    time: 0.7272  last_time: 1.5440  data_time: 0.6130  last_data_time: 1.3098   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:24:15 d2.utils.events]: \u001b[0m eta: 0:01:35  iter: 1739  total_loss: 0.948  loss_cls: 0.2276  loss_box_reg: 0.3428  loss_mask: 0.1784  loss_rpn_cls: 0.05483  loss_rpn_loc: 0.1017    time: 0.7261  last_time: 0.3625  data_time: 0.4091  last_data_time: 0.1709   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:24:31 d2.utils.events]: \u001b[0m eta: 0:01:28  iter: 1759  total_loss: 0.7252  loss_cls: 0.1999  loss_box_reg: 0.2418  loss_mask: 0.1487  loss_rpn_cls: 0.03009  loss_rpn_loc: 0.09651    time: 0.7269  last_time: 1.2872  data_time: 0.5882  last_data_time: 1.0440   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:24:46 d2.utils.events]: \u001b[0m eta: 0:01:20  iter: 1779  total_loss: 0.8855  loss_cls: 0.2408  loss_box_reg: 0.2782  loss_mask: 0.1845  loss_rpn_cls: 0.03004  loss_rpn_loc: 0.119    time: 0.7272  last_time: 0.3932  data_time: 0.5165  last_data_time: 0.1615   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:25:02 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:25:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:25:02 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:25:02 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:25:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:25:02 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:25:02 d2.utils.events]: \u001b[0m eta: 0:01:14  iter: 1799  total_loss: 0.8709  loss_cls: 0.2202  loss_box_reg: 0.3048  loss_mask: 0.1826  loss_rpn_cls: 0.04344  loss_rpn_loc: 0.117    time: 0.7277  last_time: 1.0365  data_time: 0.5361  last_data_time: 0.7873   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:25:13 d2.utils.events]: \u001b[0m eta: 0:01:08  iter: 1819  total_loss: 0.7569  loss_cls: 0.196  loss_box_reg: 0.2822  loss_mask: 0.1655  loss_rpn_cls: 0.03146  loss_rpn_loc: 0.1054    time: 0.7261  last_time: 0.6177  data_time: 0.3587  last_data_time: 0.4384   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:25:30 d2.utils.events]: \u001b[0m eta: 0:00:59  iter: 1839  total_loss: 0.9449  loss_cls: 0.234  loss_box_reg: 0.2911  loss_mask: 0.1991  loss_rpn_cls: 0.05284  loss_rpn_loc: 0.1349    time: 0.7272  last_time: 0.2531  data_time: 0.5863  last_data_time: 0.0018   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:25:44 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 1859  total_loss: 0.849  loss_cls: 0.2262  loss_box_reg: 0.3068  loss_mask: 0.1808  loss_rpn_cls: 0.02402  loss_rpn_loc: 0.1176    time: 0.7271  last_time: 1.5116  data_time: 0.4891  last_data_time: 1.2821   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:25:59 d2.utils.events]: \u001b[0m eta: 0:00:44  iter: 1879  total_loss: 0.9834  loss_cls: 0.223  loss_box_reg: 0.3281  loss_mask: 0.1872  loss_rpn_cls: 0.0368  loss_rpn_loc: 0.1398    time: 0.7270  last_time: 0.2422  data_time: 0.4751  last_data_time: 0.0111   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:26:14 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:26:14 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:26:14 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:26:14 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:26:14 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:26:14 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 13:26:14 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 1899  total_loss: 0.7997  loss_cls: 0.2065  loss_box_reg: 0.2701  loss_mask: 0.1712  loss_rpn_cls: 0.05138  loss_rpn_loc: 0.1354    time: 0.7275  last_time: 0.5280  data_time: 0.5263  last_data_time: 0.2932   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:26:26 d2.utils.events]: \u001b[0m eta: 0:00:29  iter: 1919  total_loss: 0.7545  loss_cls: 0.2081  loss_box_reg: 0.2833  loss_mask: 0.1535  loss_rpn_cls: 0.02817  loss_rpn_loc: 0.1133    time: 0.7260  last_time: 0.5166  data_time: 0.3650  last_data_time: 0.3129   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:26:39 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 1939  total_loss: 0.758  loss_cls: 0.2102  loss_box_reg: 0.265  loss_mask: 0.1692  loss_rpn_cls: 0.0253  loss_rpn_loc: 0.1175    time: 0.7253  last_time: 0.4354  data_time: 0.4215  last_data_time: 0.1859   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:26:56 d2.utils.events]: \u001b[0m eta: 0:00:14  iter: 1959  total_loss: 0.8579  loss_cls: 0.2403  loss_box_reg: 0.2496  loss_mask: 0.1917  loss_rpn_cls: 0.04742  loss_rpn_loc: 0.1102    time: 0.7266  last_time: 2.0041  data_time: 0.6318  last_data_time: 1.6981   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:27:10 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 1979  total_loss: 0.6684  loss_cls: 0.1731  loss_box_reg: 0.2573  loss_mask: 0.1617  loss_rpn_cls: 0.01914  loss_rpn_loc: 0.1056    time: 0.7263  last_time: 0.2122  data_time: 0.4642  last_data_time: 0.0049   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:27:27 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 1999  total_loss: 0.8815  loss_cls: 0.2339  loss_box_reg: 0.2578  loss_mask: 0.1827  loss_rpn_cls: 0.03478  loss_rpn_loc: 0.1217    time: 0.7266  last_time: 1.5950  data_time: 0.5402  last_data_time: 1.3630   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 13:27:27 d2.engine.hooks]: \u001b[0mOverall training speed: 1998 iterations in 0:24:11 (0.7267 s / it)\n",
      "\u001b[32m[07/19 13:27:27 d2.engine.hooks]: \u001b[0mTotal training time: 0:24:16 (0:00:04 on hooks)\n",
      "\u001b[32m[07/19 13:27:27 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 13:27:27 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 13:27:27 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 13:27:27 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 13:27:27 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 13:27:27 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trainer = MyTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you need to continue to train your model, you can use this part of the code. You can modify the NB_ITERATION.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_ITERATION = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/19 14:32:42 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[07/19 14:32:42 d2.data.datasets.coco]: \u001b[0mLoaded 109 images in COCO format from datasets/train/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:32:42 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 109 images left.\n",
      "\u001b[32m[07/19 14:32:42 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[07/19 14:32:42 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[07/19 14:32:42 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:32:42 d2.data.common]: \u001b[0mSerializing 109 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:32:42 d2.data.common]: \u001b[0mSerialized dataset takes 3.25 MiB\n",
      "\u001b[32m[07/19 14:32:42 d2.data.build]: \u001b[0mMaking batched data loader with batch_size=1\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:32:42 d2.solver.build]: \u001b[0mSOLVER.STEPS contains values larger than SOLVER.MAX_ITER. These values will be ignored.\n",
      "\u001b[32m[07/19 14:32:43 d2.checkpoint.detection_checkpoint]: \u001b[0m[DetectionCheckpointer] Loading from bloc_segmentation\\mask_rcnn_R_101_FPN_3x\\2024-07-19-13-02-30\\model_final.pth ...\n",
      "\u001b[32m[07/19 14:32:43 d2.engine.hooks]: \u001b[0mLoading scheduler from state_dict ...\n",
      "\u001b[32m[07/19 14:32:43 d2.engine.train_loop]: \u001b[0mStarting training from iteration 4000\n",
      "\u001b[32m[07/19 14:33:00 d2.utils.events]: \u001b[0m eta: 0:17:12  iter: 4019  total_loss: 0.5683  loss_cls: 0.1396  loss_box_reg: 0.2071  loss_mask: 0.1352  loss_rpn_cls: 0.008786  loss_rpn_loc: 0.08725    time: 0.6095  last_time: 0.1962  data_time: 0.6155  last_data_time: 0.0060   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 14:33:12 d2.utils.events]: \u001b[0m eta: 0:17:17  iter: 4039  total_loss: 0.6359  loss_cls: 0.1555  loss_box_reg: 0.2339  loss_mask: 0.1365  loss_rpn_cls: 0.008811  loss_rpn_loc: 0.09506    time: 0.6008  last_time: 0.5125  data_time: 0.3584  last_data_time: 0.2398   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 14:33:26 d2.utils.events]: \u001b[0m eta: 0:18:21  iter: 4059  total_loss: 0.5542  loss_cls: 0.1147  loss_box_reg: 0.2302  loss_mask: 0.1281  loss_rpn_cls: 0.01187  loss_rpn_loc: 0.08784    time: 0.6374  last_time: 0.4052  data_time: 0.4829  last_data_time: 0.2220   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 14:33:48 d2.utils.events]: \u001b[0m eta: 0:18:00  iter: 4079  total_loss: 0.6968  loss_cls: 0.1669  loss_box_reg: 0.25  loss_mask: 0.1523  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.08977    time: 0.7525  last_time: 1.4769  data_time: 0.8217  last_data_time: 1.2374   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 14:34:04 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:34:04 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:34:04 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:34:04 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:34:04 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:34:04 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:34:04 d2.utils.events]: \u001b[0m eta: 0:17:32  iter: 4099  total_loss: 0.6372  loss_cls: 0.1495  loss_box_reg: 0.2268  loss_mask: 0.1466  loss_rpn_cls: 0.01784  loss_rpn_loc: 0.08574    time: 0.7610  last_time: 0.8002  data_time: 0.5314  last_data_time: 0.6242   lr: 0.001  max_mem: 4375M\n",
      "\u001b[32m[07/19 14:34:22 d2.utils.events]: \u001b[0m eta: 0:16:57  iter: 4119  total_loss: 0.6593  loss_cls: 0.1531  loss_box_reg: 0.237  loss_mask: 0.1572  loss_rpn_cls: 0.01663  loss_rpn_loc: 0.09109    time: 0.7873  last_time: 1.4645  data_time: 0.6668  last_data_time: 1.2814   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:34:39 d2.utils.events]: \u001b[0m eta: 0:16:46  iter: 4139  total_loss: 0.7093  loss_cls: 0.1482  loss_box_reg: 0.2515  loss_mask: 0.1736  loss_rpn_cls: 0.02171  loss_rpn_loc: 0.09121    time: 0.7918  last_time: 2.5551  data_time: 0.5718  last_data_time: 2.0312   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:34:52 d2.utils.events]: \u001b[0m eta: 0:15:35  iter: 4159  total_loss: 0.6961  loss_cls: 0.1738  loss_box_reg: 0.2391  loss_mask: 0.1507  loss_rpn_cls: 0.0121  loss_rpn_loc: 0.08867    time: 0.7741  last_time: 0.2792  data_time: 0.4121  last_data_time: 0.0014   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:35:05 d2.utils.events]: \u001b[0m eta: 0:15:10  iter: 4179  total_loss: 0.5609  loss_cls: 0.1205  loss_box_reg: 0.191  loss_mask: 0.1451  loss_rpn_cls: 0.01247  loss_rpn_loc: 0.08883    time: 0.7635  last_time: 0.2711  data_time: 0.4360  last_data_time: 0.0200   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:35:20 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:35:20 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:35:20 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:35:20 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:35:20 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:35:20 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:35:20 d2.utils.events]: \u001b[0m eta: 0:15:15  iter: 4199  total_loss: 0.6041  loss_cls: 0.1273  loss_box_reg: 0.2233  loss_mask: 0.1362  loss_rpn_cls: 0.008107  loss_rpn_loc: 0.1007    time: 0.7622  last_time: 1.6497  data_time: 0.5234  last_data_time: 1.4361   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:35:35 d2.utils.events]: \u001b[0m eta: 0:14:50  iter: 4219  total_loss: 0.5324  loss_cls: 0.1224  loss_box_reg: 0.1891  loss_mask: 0.1289  loss_rpn_cls: 0.01296  loss_rpn_loc: 0.08601    time: 0.7597  last_time: 1.7456  data_time: 0.5137  last_data_time: 1.5126   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:35:48 d2.utils.events]: \u001b[0m eta: 0:14:21  iter: 4239  total_loss: 0.6142  loss_cls: 0.1327  loss_box_reg: 0.2062  loss_mask: 0.1568  loss_rpn_cls: 0.01753  loss_rpn_loc: 0.09908    time: 0.7514  last_time: 0.2123  data_time: 0.4459  last_data_time: 0.0183   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:36:02 d2.utils.events]: \u001b[0m eta: 0:14:11  iter: 4259  total_loss: 0.5841  loss_cls: 0.1479  loss_box_reg: 0.1853  loss_mask: 0.133  loss_rpn_cls: 0.007468  loss_rpn_loc: 0.08754    time: 0.7478  last_time: 0.7112  data_time: 0.4764  last_data_time: 0.4915   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:36:16 d2.utils.events]: \u001b[0m eta: 0:13:22  iter: 4279  total_loss: 0.5349  loss_cls: 0.1397  loss_box_reg: 0.1926  loss_mask: 0.1304  loss_rpn_cls: 0.01236  loss_rpn_loc: 0.08378    time: 0.7417  last_time: 3.4471  data_time: 0.4372  last_data_time: 3.0517   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:36:31 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:36:31 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:36:31 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:36:31 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:36:31 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:36:31 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:36:31 d2.utils.events]: \u001b[0m eta: 0:13:13  iter: 4299  total_loss: 0.6637  loss_cls: 0.1575  loss_box_reg: 0.26  loss_mask: 0.138  loss_rpn_cls: 0.01929  loss_rpn_loc: 0.09517    time: 0.7444  last_time: 0.6436  data_time: 0.5410  last_data_time: 0.4518   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:36:45 d2.utils.events]: \u001b[0m eta: 0:12:45  iter: 4319  total_loss: 0.6197  loss_cls: 0.1385  loss_box_reg: 0.2512  loss_mask: 0.1358  loss_rpn_cls: 0.01937  loss_rpn_loc: 0.08892    time: 0.7397  last_time: 0.2387  data_time: 0.4464  last_data_time: 0.0116   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:36:58 d2.utils.events]: \u001b[0m eta: 0:12:21  iter: 4339  total_loss: 0.5553  loss_cls: 0.1109  loss_box_reg: 0.1904  loss_mask: 0.1203  loss_rpn_cls: 0.01046  loss_rpn_loc: 0.07906    time: 0.7339  last_time: 0.6226  data_time: 0.4148  last_data_time: 0.4416   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:37:15 d2.utils.events]: \u001b[0m eta: 0:12:50  iter: 4359  total_loss: 0.57  loss_cls: 0.116  loss_box_reg: 0.2078  loss_mask: 0.1472  loss_rpn_cls: 0.01308  loss_rpn_loc: 0.08866    time: 0.7414  last_time: 0.2637  data_time: 0.6086  last_data_time: 0.0053   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:37:28 d2.utils.events]: \u001b[0m eta: 0:12:17  iter: 4379  total_loss: 0.6402  loss_cls: 0.1484  loss_box_reg: 0.2201  loss_mask: 0.1369  loss_rpn_cls: 0.01334  loss_rpn_loc: 0.0956    time: 0.7367  last_time: 0.2031  data_time: 0.4418  last_data_time: 0.0135   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:37:41 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:37:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:37:41 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:37:41 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:37:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:37:41 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:37:41 d2.utils.events]: \u001b[0m eta: 0:12:08  iter: 4399  total_loss: 0.5598  loss_cls: 0.1337  loss_box_reg: 0.2092  loss_mask: 0.1411  loss_rpn_cls: 0.01194  loss_rpn_loc: 0.09665    time: 0.7328  last_time: 0.7097  data_time: 0.4325  last_data_time: 0.4529   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:37:55 d2.utils.events]: \u001b[0m eta: 0:12:09  iter: 4419  total_loss: 0.6503  loss_cls: 0.1771  loss_box_reg: 0.2361  loss_mask: 0.1395  loss_rpn_cls: 0.01257  loss_rpn_loc: 0.0887    time: 0.7316  last_time: 0.7074  data_time: 0.4755  last_data_time: 0.5103   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:38:11 d2.utils.events]: \u001b[0m eta: 0:11:50  iter: 4439  total_loss: 0.6024  loss_cls: 0.149  loss_box_reg: 0.2102  loss_mask: 0.1351  loss_rpn_cls: 0.008449  loss_rpn_loc: 0.1047    time: 0.7330  last_time: 0.2175  data_time: 0.5220  last_data_time: 0.0055   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:38:25 d2.utils.events]: \u001b[0m eta: 0:11:20  iter: 4459  total_loss: 0.629  loss_cls: 0.1471  loss_box_reg: 0.2117  loss_mask: 0.1606  loss_rpn_cls: 0.01704  loss_rpn_loc: 0.08297    time: 0.7316  last_time: 0.3894  data_time: 0.4572  last_data_time: 0.1242   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:38:39 d2.utils.events]: \u001b[0m eta: 0:11:18  iter: 4479  total_loss: 0.5476  loss_cls: 0.1304  loss_box_reg: 0.1902  loss_mask: 0.1326  loss_rpn_cls: 0.01105  loss_rpn_loc: 0.0866    time: 0.7314  last_time: 1.5566  data_time: 0.4947  last_data_time: 1.2882   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:38:54 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:38:54 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:38:54 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:38:54 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:38:54 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:38:54 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:38:54 d2.utils.events]: \u001b[0m eta: 0:11:09  iter: 4499  total_loss: 0.5263  loss_cls: 0.1131  loss_box_reg: 0.1998  loss_mask: 0.1183  loss_rpn_cls: 0.01153  loss_rpn_loc: 0.07815    time: 0.7305  last_time: 1.1766  data_time: 0.4907  last_data_time: 0.9614   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:39:10 d2.utils.events]: \u001b[0m eta: 0:10:47  iter: 4519  total_loss: 0.5705  loss_cls: 0.1428  loss_box_reg: 0.213  loss_mask: 0.1468  loss_rpn_cls: 0.01541  loss_rpn_loc: 0.08755    time: 0.7343  last_time: 0.2480  data_time: 0.5902  last_data_time: 0.0104   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:39:25 d2.utils.events]: \u001b[0m eta: 0:10:28  iter: 4539  total_loss: 0.6739  loss_cls: 0.1809  loss_box_reg: 0.2129  loss_mask: 0.1733  loss_rpn_cls: 0.01748  loss_rpn_loc: 0.09679    time: 0.7344  last_time: 0.2204  data_time: 0.5073  last_data_time: 0.0075   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:39:39 d2.utils.events]: \u001b[0m eta: 0:10:24  iter: 4559  total_loss: 0.6162  loss_cls: 0.09967  loss_box_reg: 0.2382  loss_mask: 0.152  loss_rpn_cls: 0.01105  loss_rpn_loc: 0.08769    time: 0.7325  last_time: 0.2758  data_time: 0.4505  last_data_time: 0.0099   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:39:53 d2.utils.events]: \u001b[0m eta: 0:10:14  iter: 4579  total_loss: 0.6116  loss_cls: 0.1359  loss_box_reg: 0.2216  loss_mask: 0.171  loss_rpn_cls: 0.01575  loss_rpn_loc: 0.09576    time: 0.7318  last_time: 0.2475  data_time: 0.4745  last_data_time: 0.0085   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:40:06 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:40:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:40:06 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:40:06 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:40:06 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:40:06 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:40:06 d2.utils.events]: \u001b[0m eta: 0:10:08  iter: 4599  total_loss: 0.6168  loss_cls: 0.1553  loss_box_reg: 0.241  loss_mask: 0.1562  loss_rpn_cls: 0.01284  loss_rpn_loc: 0.08725    time: 0.7287  last_time: 1.1796  data_time: 0.4038  last_data_time: 0.9849   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:40:25 d2.utils.events]: \u001b[0m eta: 0:09:59  iter: 4619  total_loss: 0.4988  loss_cls: 0.08675  loss_box_reg: 0.2091  loss_mask: 0.1245  loss_rpn_cls: 0.0108  loss_rpn_loc: 0.08787    time: 0.7359  last_time: 0.6878  data_time: 0.7126  last_data_time: 0.4180   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:40:38 d2.utils.events]: \u001b[0m eta: 0:09:48  iter: 4639  total_loss: 0.606  loss_cls: 0.1437  loss_box_reg: 0.2166  loss_mask: 0.1365  loss_rpn_cls: 0.008993  loss_rpn_loc: 0.09616    time: 0.7342  last_time: 1.5930  data_time: 0.4495  last_data_time: 1.3417   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:40:55 d2.utils.events]: \u001b[0m eta: 0:09:34  iter: 4659  total_loss: 0.6798  loss_cls: 0.1463  loss_box_reg: 0.2248  loss_mask: 0.1537  loss_rpn_cls: 0.01922  loss_rpn_loc: 0.08549    time: 0.7370  last_time: 0.2678  data_time: 0.5655  last_data_time: 0.0518   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:41:06 d2.utils.events]: \u001b[0m eta: 0:09:18  iter: 4679  total_loss: 0.609  loss_cls: 0.1443  loss_box_reg: 0.2226  loss_mask: 0.1309  loss_rpn_cls: 0.008942  loss_rpn_loc: 0.08496    time: 0.7320  last_time: 0.2465  data_time: 0.3280  last_data_time: 0.0073   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:41:21 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:41:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:41:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:41:21 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:41:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:41:21 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:41:21 d2.utils.events]: \u001b[0m eta: 0:09:02  iter: 4699  total_loss: 0.5024  loss_cls: 0.1158  loss_box_reg: 0.1831  loss_mask: 0.1299  loss_rpn_cls: 0.009616  loss_rpn_loc: 0.07963    time: 0.7327  last_time: 0.2092  data_time: 0.5361  last_data_time: 0.0052   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:41:34 d2.utils.events]: \u001b[0m eta: 0:08:59  iter: 4719  total_loss: 0.5755  loss_cls: 0.1261  loss_box_reg: 0.2226  loss_mask: 0.1476  loss_rpn_cls: 0.007224  loss_rpn_loc: 0.08694    time: 0.7294  last_time: 0.2963  data_time: 0.3753  last_data_time: 0.0022   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:41:49 d2.utils.events]: \u001b[0m eta: 0:08:56  iter: 4739  total_loss: 0.6542  loss_cls: 0.1667  loss_box_reg: 0.2179  loss_mask: 0.1403  loss_rpn_cls: 0.007493  loss_rpn_loc: 0.09673    time: 0.7309  last_time: 1.4856  data_time: 0.5575  last_data_time: 1.2763   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:42:04 d2.utils.events]: \u001b[0m eta: 0:08:48  iter: 4759  total_loss: 0.6539  loss_cls: 0.1238  loss_box_reg: 0.2049  loss_mask: 0.134  loss_rpn_cls: 0.009031  loss_rpn_loc: 0.08462    time: 0.7311  last_time: 1.3959  data_time: 0.5031  last_data_time: 1.2021   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:42:17 d2.utils.events]: \u001b[0m eta: 0:08:42  iter: 4779  total_loss: 0.6412  loss_cls: 0.1133  loss_box_reg: 0.2007  loss_mask: 0.1439  loss_rpn_cls: 0.009777  loss_rpn_loc: 0.08918    time: 0.7290  last_time: 0.5673  data_time: 0.4212  last_data_time: 0.3335   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:42:33 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:42:33 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:42:33 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:42:33 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:42:33 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:42:33 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:42:33 d2.utils.events]: \u001b[0m eta: 0:08:36  iter: 4799  total_loss: 0.6627  loss_cls: 0.1536  loss_box_reg: 0.2543  loss_mask: 0.1493  loss_rpn_cls: 0.02804  loss_rpn_loc: 0.09816    time: 0.7309  last_time: 0.5672  data_time: 0.5550  last_data_time: 0.3576   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:42:48 d2.utils.events]: \u001b[0m eta: 0:08:36  iter: 4819  total_loss: 0.5727  loss_cls: 0.1227  loss_box_reg: 0.2608  loss_mask: 0.1395  loss_rpn_cls: 0.01103  loss_rpn_loc: 0.07698    time: 0.7305  last_time: 0.6224  data_time: 0.4752  last_data_time: 0.3549   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:43:03 d2.utils.events]: \u001b[0m eta: 0:08:25  iter: 4839  total_loss: 0.5955  loss_cls: 0.1516  loss_box_reg: 0.2083  loss_mask: 0.1428  loss_rpn_cls: 0.01921  loss_rpn_loc: 0.08743    time: 0.7311  last_time: 0.2624  data_time: 0.5101  last_data_time: 0.0250   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:43:16 d2.utils.events]: \u001b[0m eta: 0:08:19  iter: 4859  total_loss: 0.6015  loss_cls: 0.1446  loss_box_reg: 0.2101  loss_mask: 0.1312  loss_rpn_cls: 0.007186  loss_rpn_loc: 0.08992    time: 0.7295  last_time: 0.2548  data_time: 0.4322  last_data_time: 0.0101   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:43:31 d2.utils.events]: \u001b[0m eta: 0:08:06  iter: 4879  total_loss: 0.6252  loss_cls: 0.1412  loss_box_reg: 0.2454  loss_mask: 0.1344  loss_rpn_cls: 0.01529  loss_rpn_loc: 0.09442    time: 0.7304  last_time: 1.3695  data_time: 0.5312  last_data_time: 1.1444   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:43:46 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:43:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:43:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:43:46 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:43:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:43:46 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:43:46 d2.utils.events]: \u001b[0m eta: 0:07:58  iter: 4899  total_loss: 0.5279  loss_cls: 0.1022  loss_box_reg: 0.1989  loss_mask: 0.1066  loss_rpn_cls: 0.004862  loss_rpn_loc: 0.08756    time: 0.7308  last_time: 1.0210  data_time: 0.5141  last_data_time: 0.8222   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:43:59 d2.utils.events]: \u001b[0m eta: 0:07:49  iter: 4919  total_loss: 0.5473  loss_cls: 0.1473  loss_box_reg: 0.1973  loss_mask: 0.1211  loss_rpn_cls: 0.005178  loss_rpn_loc: 0.08149    time: 0.7282  last_time: 1.2383  data_time: 0.3756  last_data_time: 1.0462   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:44:11 d2.utils.events]: \u001b[0m eta: 0:07:40  iter: 4939  total_loss: 0.5656  loss_cls: 0.1385  loss_box_reg: 0.1905  loss_mask: 0.1431  loss_rpn_cls: 0.01157  loss_rpn_loc: 0.0794    time: 0.7262  last_time: 1.5375  data_time: 0.4015  last_data_time: 1.3000   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:44:25 d2.utils.events]: \u001b[0m eta: 0:07:32  iter: 4959  total_loss: 0.507  loss_cls: 0.1197  loss_box_reg: 0.1735  loss_mask: 0.1272  loss_rpn_cls: 0.006553  loss_rpn_loc: 0.07603    time: 0.7255  last_time: 0.5305  data_time: 0.4461  last_data_time: 0.3293   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:44:43 d2.utils.events]: \u001b[0m eta: 0:07:34  iter: 4979  total_loss: 0.7434  loss_cls: 0.1485  loss_box_reg: 0.2834  loss_mask: 0.1749  loss_rpn_cls: 0.02111  loss_rpn_loc: 0.1059    time: 0.7290  last_time: 0.8337  data_time: 0.6345  last_data_time: 0.6188   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:44:58 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:44:58 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:44:58 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:44:58 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:44:58 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:44:58 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:44:58 d2.utils.events]: \u001b[0m eta: 0:07:24  iter: 4999  total_loss: 0.5319  loss_cls: 0.1239  loss_box_reg: 0.1843  loss_mask: 0.1255  loss_rpn_cls: 0.006148  loss_rpn_loc: 0.07483    time: 0.7283  last_time: 1.7603  data_time: 0.4694  last_data_time: 1.5144   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:45:12 d2.utils.events]: \u001b[0m eta: 0:07:09  iter: 5019  total_loss: 0.6458  loss_cls: 0.1463  loss_box_reg: 0.2253  loss_mask: 0.1407  loss_rpn_cls: 0.0169  loss_rpn_loc: 0.09206    time: 0.7284  last_time: 1.2111  data_time: 0.4951  last_data_time: 0.9382   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:45:25 d2.utils.events]: \u001b[0m eta: 0:06:55  iter: 5039  total_loss: 0.601  loss_cls: 0.1287  loss_box_reg: 0.2158  loss_mask: 0.127  loss_rpn_cls: 0.01355  loss_rpn_loc: 0.09235    time: 0.7268  last_time: 0.2504  data_time: 0.4120  last_data_time: 0.0060   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:45:40 d2.utils.events]: \u001b[0m eta: 0:06:38  iter: 5059  total_loss: 0.531  loss_cls: 0.09707  loss_box_reg: 0.2003  loss_mask: 0.1302  loss_rpn_cls: 0.01302  loss_rpn_loc: 0.07799    time: 0.7270  last_time: 0.1994  data_time: 0.5026  last_data_time: 0.0029   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:45:56 d2.utils.events]: \u001b[0m eta: 0:06:28  iter: 5079  total_loss: 0.6162  loss_cls: 0.156  loss_box_reg: 0.2181  loss_mask: 0.1543  loss_rpn_cls: 0.01929  loss_rpn_loc: 0.09188    time: 0.7286  last_time: 0.2920  data_time: 0.5721  last_data_time: 0.0242   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:46:11 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:46:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:46:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:46:11 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:46:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:46:11 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:46:11 d2.utils.events]: \u001b[0m eta: 0:06:17  iter: 5099  total_loss: 0.4797  loss_cls: 0.08345  loss_box_reg: 0.2054  loss_mask: 0.1345  loss_rpn_cls: 0.005039  loss_rpn_loc: 0.08406    time: 0.7282  last_time: 1.4540  data_time: 0.4782  last_data_time: 1.2058   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:46:25 d2.utils.events]: \u001b[0m eta: 0:06:08  iter: 5119  total_loss: 0.5719  loss_cls: 0.1131  loss_box_reg: 0.1933  loss_mask: 0.1244  loss_rpn_cls: 0.0101  loss_rpn_loc: 0.1473    time: 0.7282  last_time: 0.2347  data_time: 0.4958  last_data_time: 0.0271   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:46:37 d2.utils.events]: \u001b[0m eta: 0:06:02  iter: 5139  total_loss: 0.5492  loss_cls: 0.08031  loss_box_reg: 0.2024  loss_mask: 0.1339  loss_rpn_cls: 0.01469  loss_rpn_loc: 0.08377    time: 0.7257  last_time: 0.2479  data_time: 0.3513  last_data_time: 0.0083   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:46:50 d2.utils.events]: \u001b[0m eta: 0:05:56  iter: 5159  total_loss: 0.5423  loss_cls: 0.1136  loss_box_reg: 0.2073  loss_mask: 0.1373  loss_rpn_cls: 0.01432  loss_rpn_loc: 0.08317    time: 0.7247  last_time: 0.5213  data_time: 0.4305  last_data_time: 0.2747   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:47:09 d2.utils.events]: \u001b[0m eta: 0:05:47  iter: 5179  total_loss: 0.6  loss_cls: 0.1537  loss_box_reg: 0.2071  loss_mask: 0.1461  loss_rpn_cls: 0.01041  loss_rpn_loc: 0.08648    time: 0.7282  last_time: 0.2007  data_time: 0.6751  last_data_time: 0.0081   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:47:24 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:47:24 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:47:24 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:47:24 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:47:24 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:47:24 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:47:24 d2.utils.events]: \u001b[0m eta: 0:05:35  iter: 5199  total_loss: 0.5556  loss_cls: 0.1293  loss_box_reg: 0.1839  loss_mask: 0.1373  loss_rpn_cls: 0.00993  loss_rpn_loc: 0.07719    time: 0.7284  last_time: 0.2547  data_time: 0.4882  last_data_time: 0.0119   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:47:39 d2.utils.events]: \u001b[0m eta: 0:05:29  iter: 5219  total_loss: 0.5108  loss_cls: 0.1009  loss_box_reg: 0.1937  loss_mask: 0.1301  loss_rpn_cls: 0.01065  loss_rpn_loc: 0.08196    time: 0.7286  last_time: 0.1947  data_time: 0.4841  last_data_time: 0.0020   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:47:57 d2.utils.events]: \u001b[0m eta: 0:05:22  iter: 5239  total_loss: 0.5303  loss_cls: 0.1301  loss_box_reg: 0.1873  loss_mask: 0.1191  loss_rpn_cls: 0.01112  loss_rpn_loc: 0.09901    time: 0.7315  last_time: 1.4007  data_time: 0.6467  last_data_time: 1.1619   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:48:10 d2.utils.events]: \u001b[0m eta: 0:05:12  iter: 5259  total_loss: 0.6051  loss_cls: 0.1588  loss_box_reg: 0.2005  loss_mask: 0.1608  loss_rpn_cls: 0.01612  loss_rpn_loc: 0.09091    time: 0.7306  last_time: 0.2337  data_time: 0.4475  last_data_time: 0.0050   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:48:25 d2.utils.events]: \u001b[0m eta: 0:05:07  iter: 5279  total_loss: 0.5356  loss_cls: 0.1004  loss_box_reg: 0.2202  loss_mask: 0.1441  loss_rpn_cls: 0.01026  loss_rpn_loc: 0.09756    time: 0.7303  last_time: 0.2590  data_time: 0.4698  last_data_time: 0.0050   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:48:37 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:48:37 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:48:37 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:48:37 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:48:37 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:48:37 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:48:37 d2.utils.events]: \u001b[0m eta: 0:04:58  iter: 5299  total_loss: 0.5929  loss_cls: 0.1449  loss_box_reg: 0.2035  loss_mask: 0.1437  loss_rpn_cls: 0.006466  loss_rpn_loc: 0.07724    time: 0.7286  last_time: 0.2394  data_time: 0.3939  last_data_time: 0.0049   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:48:49 d2.utils.events]: \u001b[0m eta: 0:04:48  iter: 5319  total_loss: 0.5006  loss_cls: 0.1195  loss_box_reg: 0.1896  loss_mask: 0.1187  loss_rpn_cls: 0.006547  loss_rpn_loc: 0.0809    time: 0.7268  last_time: 0.2142  data_time: 0.3640  last_data_time: 0.0097   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:49:04 d2.utils.events]: \u001b[0m eta: 0:04:44  iter: 5339  total_loss: 0.5131  loss_cls: 0.1034  loss_box_reg: 0.1815  loss_mask: 0.1247  loss_rpn_cls: 0.006847  loss_rpn_loc: 0.08908    time: 0.7268  last_time: 0.4617  data_time: 0.4970  last_data_time: 0.1977   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:49:17 d2.utils.events]: \u001b[0m eta: 0:04:33  iter: 5359  total_loss: 0.5829  loss_cls: 0.1592  loss_box_reg: 0.2014  loss_mask: 0.1234  loss_rpn_cls: 0.01082  loss_rpn_loc: 0.08143    time: 0.7261  last_time: 0.9920  data_time: 0.4416  last_data_time: 0.6823   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:49:30 d2.utils.events]: \u001b[0m eta: 0:04:29  iter: 5379  total_loss: 0.5271  loss_cls: 0.1429  loss_box_reg: 0.1847  loss_mask: 0.1107  loss_rpn_cls: 0.007704  loss_rpn_loc: 0.07885    time: 0.7250  last_time: 0.2075  data_time: 0.4097  last_data_time: 0.0162   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:49:46 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:49:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:49:46 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:49:46 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:49:46 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:49:46 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:49:46 d2.utils.events]: \u001b[0m eta: 0:04:20  iter: 5399  total_loss: 0.6429  loss_cls: 0.1402  loss_box_reg: 0.2137  loss_mask: 0.1605  loss_rpn_cls: 0.01649  loss_rpn_loc: 0.08066    time: 0.7260  last_time: 0.1964  data_time: 0.5521  last_data_time: 0.0067   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:49:59 d2.utils.events]: \u001b[0m eta: 0:04:08  iter: 5419  total_loss: 0.6211  loss_cls: 0.1373  loss_box_reg: 0.202  loss_mask: 0.1649  loss_rpn_cls: 0.008924  loss_rpn_loc: 0.08182    time: 0.7243  last_time: 0.7056  data_time: 0.3628  last_data_time: 0.4563   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:50:14 d2.utils.events]: \u001b[0m eta: 0:03:59  iter: 5439  total_loss: 0.5207  loss_cls: 0.1307  loss_box_reg: 0.1861  loss_mask: 0.1266  loss_rpn_cls: 0.006984  loss_rpn_loc: 0.08226    time: 0.7248  last_time: 0.2310  data_time: 0.5241  last_data_time: 0.0117   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:50:30 d2.utils.events]: \u001b[0m eta: 0:03:50  iter: 5459  total_loss: 0.6523  loss_cls: 0.1227  loss_box_reg: 0.2198  loss_mask: 0.143  loss_rpn_cls: 0.0118  loss_rpn_loc: 0.09429    time: 0.7260  last_time: 0.2794  data_time: 0.5599  last_data_time: 0.0079   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:50:44 d2.utils.events]: \u001b[0m eta: 0:03:42  iter: 5479  total_loss: 0.5458  loss_cls: 0.1138  loss_box_reg: 0.2089  loss_mask: 0.123  loss_rpn_cls: 0.01245  loss_rpn_loc: 0.08879    time: 0.7258  last_time: 0.6565  data_time: 0.4833  last_data_time: 0.3984   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:50:57 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:50:57 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:50:57 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:50:57 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:50:57 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:50:57 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:50:57 d2.utils.events]: \u001b[0m eta: 0:03:35  iter: 5499  total_loss: 0.5718  loss_cls: 0.1162  loss_box_reg: 0.2158  loss_mask: 0.1481  loss_rpn_cls: 0.01072  loss_rpn_loc: 0.0808    time: 0.7248  last_time: 0.5472  data_time: 0.4275  last_data_time: 0.3440   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:51:10 d2.utils.events]: \u001b[0m eta: 0:03:28  iter: 5519  total_loss: 0.524  loss_cls: 0.1265  loss_box_reg: 0.203  loss_mask: 0.1317  loss_rpn_cls: 0.01037  loss_rpn_loc: 0.07628    time: 0.7237  last_time: 0.9207  data_time: 0.4089  last_data_time: 0.6650   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:51:24 d2.utils.events]: \u001b[0m eta: 0:03:20  iter: 5539  total_loss: 0.5514  loss_cls: 0.1119  loss_box_reg: 0.2029  loss_mask: 0.1493  loss_rpn_cls: 0.005668  loss_rpn_loc: 0.07661    time: 0.7231  last_time: 0.2845  data_time: 0.4339  last_data_time: 0.0449   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:51:36 d2.utils.events]: \u001b[0m eta: 0:03:11  iter: 5559  total_loss: 0.5054  loss_cls: 0.09305  loss_box_reg: 0.2033  loss_mask: 0.1269  loss_rpn_cls: 0.009862  loss_rpn_loc: 0.08604    time: 0.7218  last_time: 0.2559  data_time: 0.4003  last_data_time: 0.0261   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:51:54 d2.utils.events]: \u001b[0m eta: 0:03:00  iter: 5579  total_loss: 0.6697  loss_cls: 0.1346  loss_box_reg: 0.221  loss_mask: 0.1472  loss_rpn_cls: 0.008817  loss_rpn_loc: 0.0935    time: 0.7237  last_time: 0.2741  data_time: 0.5966  last_data_time: 0.0083   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:52:07 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:52:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:52:07 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:52:07 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:52:07 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:52:07 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:52:07 d2.utils.events]: \u001b[0m eta: 0:02:51  iter: 5599  total_loss: 0.5726  loss_cls: 0.1249  loss_box_reg: 0.2052  loss_mask: 0.1362  loss_rpn_cls: 0.005725  loss_rpn_loc: 0.08328    time: 0.7230  last_time: 0.4396  data_time: 0.4511  last_data_time: 0.2375   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:52:20 d2.utils.events]: \u001b[0m eta: 0:02:42  iter: 5619  total_loss: 0.5409  loss_cls: 0.1029  loss_box_reg: 0.186  loss_mask: 0.1311  loss_rpn_cls: 0.009946  loss_rpn_loc: 0.08614    time: 0.7220  last_time: 1.8977  data_time: 0.4040  last_data_time: 1.6565   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:52:34 d2.utils.events]: \u001b[0m eta: 0:02:33  iter: 5639  total_loss: 0.5333  loss_cls: 0.1236  loss_box_reg: 0.1671  loss_mask: 0.1414  loss_rpn_cls: 0.006751  loss_rpn_loc: 0.09146    time: 0.7219  last_time: 1.3883  data_time: 0.4831  last_data_time: 1.1730   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:52:51 d2.utils.events]: \u001b[0m eta: 0:02:25  iter: 5659  total_loss: 0.568  loss_cls: 0.125  loss_box_reg: 0.1768  loss_mask: 0.1296  loss_rpn_cls: 0.01713  loss_rpn_loc: 0.08452    time: 0.7231  last_time: 0.2298  data_time: 0.5634  last_data_time: 0.0180   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:53:05 d2.utils.events]: \u001b[0m eta: 0:02:16  iter: 5679  total_loss: 0.6104  loss_cls: 0.1467  loss_box_reg: 0.2091  loss_mask: 0.1576  loss_rpn_cls: 0.01542  loss_rpn_loc: 0.08293    time: 0.7229  last_time: 1.3194  data_time: 0.4649  last_data_time: 1.1117   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:53:21 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:53:21 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:53:21 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:53:21 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:53:21 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:53:21 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:53:21 d2.utils.events]: \u001b[0m eta: 0:02:08  iter: 5699  total_loss: 0.6048  loss_cls: 0.152  loss_box_reg: 0.2225  loss_mask: 0.1378  loss_rpn_cls: 0.01138  loss_rpn_loc: 0.09512    time: 0.7237  last_time: 0.2147  data_time: 0.5563  last_data_time: 0.0012   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:53:35 d2.utils.events]: \u001b[0m eta: 0:01:59  iter: 5719  total_loss: 0.5373  loss_cls: 0.1406  loss_box_reg: 0.1968  loss_mask: 0.1317  loss_rpn_cls: 0.008227  loss_rpn_loc: 0.0799    time: 0.7237  last_time: 0.2778  data_time: 0.4793  last_data_time: 0.0276   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:53:48 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 5739  total_loss: 0.5325  loss_cls: 0.1112  loss_box_reg: 0.2007  loss_mask: 0.1209  loss_rpn_cls: 0.009198  loss_rpn_loc: 0.08152    time: 0.7226  last_time: 0.2627  data_time: 0.3994  last_data_time: 0.0105   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:54:05 d2.utils.events]: \u001b[0m eta: 0:01:41  iter: 5759  total_loss: 0.5326  loss_cls: 0.1022  loss_box_reg: 0.1958  loss_mask: 0.1203  loss_rpn_cls: 0.008985  loss_rpn_loc: 0.1094    time: 0.7244  last_time: 3.9153  data_time: 0.6392  last_data_time: 3.5038   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:54:17 d2.utils.events]: \u001b[0m eta: 0:01:32  iter: 5779  total_loss: 0.5813  loss_cls: 0.1102  loss_box_reg: 0.1992  loss_mask: 0.1395  loss_rpn_cls: 0.01179  loss_rpn_loc: 0.07968    time: 0.7226  last_time: 0.9496  data_time: 0.3412  last_data_time: 0.6837   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:54:35 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:54:35 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:54:35 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:54:35 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:54:35 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:54:35 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:54:35 d2.utils.events]: \u001b[0m eta: 0:01:24  iter: 5799  total_loss: 0.551  loss_cls: 0.1319  loss_box_reg: 0.2071  loss_mask: 0.1423  loss_rpn_cls: 0.008106  loss_rpn_loc: 0.07842    time: 0.7247  last_time: 1.6572  data_time: 0.6706  last_data_time: 1.4113   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:54:49 d2.utils.events]: \u001b[0m eta: 0:01:11  iter: 5819  total_loss: 0.5357  loss_cls: 0.1101  loss_box_reg: 0.1948  loss_mask: 0.1301  loss_rpn_cls: 0.008341  loss_rpn_loc: 0.08636    time: 0.7246  last_time: 0.2389  data_time: 0.4757  last_data_time: 0.0039   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:55:02 d2.utils.events]: \u001b[0m eta: 0:01:03  iter: 5839  total_loss: 0.5118  loss_cls: 0.08829  loss_box_reg: 0.2085  loss_mask: 0.1587  loss_rpn_cls: 0.005111  loss_rpn_loc: 0.0731    time: 0.7238  last_time: 0.2856  data_time: 0.4012  last_data_time: 0.0018   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:55:14 d2.utils.events]: \u001b[0m eta: 0:00:53  iter: 5859  total_loss: 0.5141  loss_cls: 0.09472  loss_box_reg: 0.1813  loss_mask: 0.1232  loss_rpn_cls: 0.006494  loss_rpn_loc: 0.0818    time: 0.7221  last_time: 0.2037  data_time: 0.3349  last_data_time: 0.0013   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:55:29 d2.utils.events]: \u001b[0m eta: 0:00:49  iter: 5879  total_loss: 0.5434  loss_cls: 0.1197  loss_box_reg: 0.2032  loss_mask: 0.1602  loss_rpn_cls: 0.01059  loss_rpn_loc: 0.0869    time: 0.7227  last_time: 0.7152  data_time: 0.5435  last_data_time: 0.4879   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:55:45 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:55:45 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:55:45 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:55:45 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:55:45 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:55:45 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n",
      "\u001b[32m[07/19 14:55:45 d2.utils.events]: \u001b[0m eta: 0:00:40  iter: 5899  total_loss: 0.66  loss_cls: 0.135  loss_box_reg: 0.228  loss_mask: 0.1587  loss_rpn_cls: 0.01015  loss_rpn_loc: 0.08316    time: 0.7236  last_time: 0.2354  data_time: 0.5612  last_data_time: 0.0112   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:55:59 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 5919  total_loss: 0.6142  loss_cls: 0.1353  loss_box_reg: 0.1975  loss_mask: 0.1442  loss_rpn_cls: 0.01424  loss_rpn_loc: 0.08503    time: 0.7233  last_time: 0.2821  data_time: 0.4480  last_data_time: 0.0062   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:56:14 d2.utils.events]: \u001b[0m eta: 0:00:24  iter: 5939  total_loss: 0.472  loss_cls: 0.09753  loss_box_reg: 0.1807  loss_mask: 0.1059  loss_rpn_cls: 0.009697  loss_rpn_loc: 0.084    time: 0.7236  last_time: 1.1955  data_time: 0.5115  last_data_time: 0.9827   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:56:28 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 5959  total_loss: 0.5051  loss_cls: 0.1243  loss_box_reg: 0.1774  loss_mask: 0.1215  loss_rpn_cls: 0.01041  loss_rpn_loc: 0.06965    time: 0.7229  last_time: 0.2119  data_time: 0.4322  last_data_time: 0.0067   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:56:41 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 5979  total_loss: 0.5571  loss_cls: 0.1093  loss_box_reg: 0.2108  loss_mask: 0.1432  loss_rpn_cls: 0.007486  loss_rpn_loc: 0.07732    time: 0.7223  last_time: 0.4830  data_time: 0.4331  last_data_time: 0.3012   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:56:56 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 5999  total_loss: 0.4712  loss_cls: 0.08692  loss_box_reg: 0.1676  loss_mask: 0.1079  loss_rpn_cls: 0.006096  loss_rpn_loc: 0.07988    time: 0.7221  last_time: 0.6920  data_time: 0.4811  last_data_time: 0.4859   lr: 0.001  max_mem: 4376M\n",
      "\u001b[32m[07/19 14:56:56 d2.engine.hooks]: \u001b[0mOverall training speed: 1998 iterations in 0:24:02 (0.7221 s / it)\n",
      "\u001b[32m[07/19 14:56:56 d2.engine.hooks]: \u001b[0mTotal training time: 0:24:07 (0:00:04 on hooks)\n",
      "\u001b[32m[07/19 14:56:56 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 14:56:56 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 14:56:56 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 14:56:56 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 14:56:56 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[07/19 14:56:56 d2.engine.defaults]: \u001b[0mNo evaluator found. Use `DefaultTrainer.test(evaluators=)`, or implement its `build_evaluator` method.\n"
     ]
    }
   ],
   "source": [
    "MAX_ITER = MAX_ITER + NB_ITERATION\n",
    "cfg.MODEL.WEIGHTS = os.path.join(OUTPUT_DIR_PATH, 'model_final.pth')  # Chemin vers le dernier checkpoint\n",
    "cfg.SOLVER.MAX_ITER = MAX_ITER\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To evaluate the model, you can monitor the loss function with tensorboard.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can evaluate the mean average precision (MAP).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Checkpoint bloc_segmentation\\mask_rcnn_R_101_FPN_3x\\2024-08-26-19-13-40\\model_final.pth not found!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mWEIGHTS \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR_PATH, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_final.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# path to the model we just trained 2024-07-19-13-02-30\u001b[39;00m\n\u001b[0;32m      2\u001b[0m cfg\u001b[38;5;241m.\u001b[39mMODEL\u001b[38;5;241m.\u001b[39mROI_HEADS\u001b[38;5;241m.\u001b[39mSCORE_THRESH_TEST \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m   \u001b[38;5;66;03m# set a custom testing threshold\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mDefaultPredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\myenv\\Lib\\site-packages\\detectron2\\engine\\defaults.py:288\u001b[0m, in \u001b[0;36mDefaultPredictor.__init__\u001b[1;34m(self, cfg)\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m MetadataCatalog\u001b[38;5;241m.\u001b[39mget(cfg\u001b[38;5;241m.\u001b[39mDATASETS\u001b[38;5;241m.\u001b[39mTEST[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    287\u001b[0m checkpointer \u001b[38;5;241m=\u001b[39m DetectionCheckpointer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m--> 288\u001b[0m \u001b[43mcheckpointer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMODEL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWEIGHTS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maug \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResizeShortestEdge(\n\u001b[0;32m    291\u001b[0m     [cfg\u001b[38;5;241m.\u001b[39mINPUT\u001b[38;5;241m.\u001b[39mMIN_SIZE_TEST, cfg\u001b[38;5;241m.\u001b[39mINPUT\u001b[38;5;241m.\u001b[39mMIN_SIZE_TEST], cfg\u001b[38;5;241m.\u001b[39mINPUT\u001b[38;5;241m.\u001b[39mMAX_SIZE_TEST\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_format \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mINPUT\u001b[38;5;241m.\u001b[39mFORMAT\n",
      "File \u001b[1;32mc:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\myenv\\Lib\\site-packages\\detectron2\\checkpoint\\detection_checkpoint.py:62\u001b[0m, in \u001b[0;36mDetectionCheckpointer.load\u001b[1;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[0;32m     60\u001b[0m     path \u001b[38;5;241m=\u001b[39m parsed_url\u001b[38;5;241m.\u001b[39m_replace(query\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgeturl()  \u001b[38;5;66;03m# remove query from filename\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_manager\u001b[38;5;241m.\u001b[39mget_local_path(path)\n\u001b[1;32m---> 62\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_sync:\n\u001b[0;32m     65\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBroadcasting model states from main worker ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\myenv\\Lib\\site-packages\\fvcore\\common\\checkpoint.py:153\u001b[0m, in \u001b[0;36mCheckpointer.load\u001b[1;34m(self, path, checkpointables)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path):\n\u001b[0;32m    152\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_manager\u001b[38;5;241m.\u001b[39mget_local_path(path)\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(path), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path)\n\u001b[0;32m    155\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_file(path)\n\u001b[0;32m    156\u001b[0m incompatible \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(checkpoint)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Checkpoint bloc_segmentation\\mask_rcnn_R_101_FPN_3x\\2024-08-26-19-13-40\\model_final.pth not found!"
     ]
    }
   ],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(OUTPUT_DIR_PATH, 'model_final.pth')  # path to the model we just trained 2024-07-19-13-02-30\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The 2 part below take into account the loading of the model, you can use them if you leave or restart the jupyter after the training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision with data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[07/19 15:03:11 d2.data.datasets.coco]: \u001b[0mLoaded 28 images in COCO format from datasets/test/annotations/instances_default.json\n",
      "\u001b[32m[07/19 15:03:11 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[07/19 15:03:11 d2.data.common]: \u001b[0mSerializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
      "\u001b[32m[07/19 15:03:11 d2.data.common]: \u001b[0mSerializing 28 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[07/19 15:03:11 d2.data.common]: \u001b[0mSerialized dataset takes 0.82 MiB\n",
      "\u001b[32m[07/19 15:03:11 d2.evaluation.evaluator]: \u001b[0mStart inference on 28 batches\n",
      "\u001b[32m[07/19 15:03:29 d2.evaluation.evaluator]: \u001b[0mInference done 1/28. Dataloading: 6.5550 s/iter. Inference: 5.8218 s/iter. Eval: 5.8628 s/iter. Total: 18.2406 s/iter. ETA=0:08:12\n",
      "\u001b[32m[07/19 15:04:25 d2.evaluation.evaluator]: \u001b[0mInference done 2/28. Dataloading: 3.2778 s/iter. Inference: 17.5274 s/iter. Eval: 16.4392 s/iter. Total: 37.2452 s/iter. ETA=0:16:08\n",
      "\u001b[32m[07/19 15:05:19 d2.evaluation.evaluator]: \u001b[0mInference done 3/28. Dataloading: 2.1856 s/iter. Inference: 21.1671 s/iter. Eval: 19.3295 s/iter. Total: 42.6830 s/iter. ETA=0:17:47\n",
      "\u001b[32m[07/19 15:05:57 d2.evaluation.evaluator]: \u001b[0mInference done 4/28. Dataloading: 1.6393 s/iter. Inference: 18.3959 s/iter. Eval: 21.5802 s/iter. Total: 41.6161 s/iter. ETA=0:16:38\n",
      "\u001b[32m[07/19 15:06:12 d2.evaluation.evaluator]: \u001b[0mInference done 5/28. Dataloading: 1.3117 s/iter. Inference: 15.2076 s/iter. Eval: 19.6894 s/iter. Total: 36.2094 s/iter. ETA=0:13:52\n",
      "\u001b[32m[07/19 15:07:07 d2.evaluation.evaluator]: \u001b[0mInference done 7/28. Dataloading: 0.0004 s/iter. Inference: 10.5219 s/iter. Eval: 16.8973 s/iter. Total: 27.4196 s/iter. ETA=0:09:35\n",
      "\u001b[32m[07/19 15:07:50 d2.evaluation.evaluator]: \u001b[0mInference done 8/28. Dataloading: 0.0006 s/iter. Inference: 10.0456 s/iter. Eval: 22.4913 s/iter. Total: 32.5377 s/iter. ETA=0:10:50\n",
      "\u001b[32m[07/19 15:08:04 d2.evaluation.evaluator]: \u001b[0mInference done 9/28. Dataloading: 0.0007 s/iter. Inference: 7.9150 s/iter. Eval: 19.9958 s/iter. Total: 27.9119 s/iter. ETA=0:08:50\n",
      "\u001b[32m[07/19 15:08:31 d2.evaluation.evaluator]: \u001b[0mInference done 10/28. Dataloading: 0.0007 s/iter. Inference: 7.2644 s/iter. Eval: 20.6227 s/iter. Total: 27.8882 s/iter. ETA=0:08:21\n",
      "\u001b[32m[07/19 15:09:23 d2.evaluation.evaluator]: \u001b[0mInference done 11/28. Dataloading: 0.0008 s/iter. Inference: 10.0977 s/iter. Eval: 21.8211 s/iter. Total: 31.9202 s/iter. ETA=0:09:02\n",
      "\u001b[32m[07/19 15:09:36 d2.evaluation.evaluator]: \u001b[0mInference done 12/28. Dataloading: 0.0009 s/iter. Inference: 8.8962 s/iter. Eval: 20.2120 s/iter. Total: 29.1097 s/iter. ETA=0:07:45\n",
      "\u001b[32m[07/19 15:09:42 d2.evaluation.evaluator]: \u001b[0mInference done 13/28. Dataloading: 0.0009 s/iter. Inference: 7.9190 s/iter. Eval: 18.3713 s/iter. Total: 26.2920 s/iter. ETA=0:06:34\n",
      "\u001b[32m[07/19 15:09:50 d2.evaluation.evaluator]: \u001b[0mInference done 15/28. Dataloading: 0.0009 s/iter. Inference: 6.4771 s/iter. Eval: 15.3067 s/iter. Total: 21.7853 s/iter. ETA=0:04:43\n",
      "\u001b[32m[07/19 15:10:08 d2.evaluation.evaluator]: \u001b[0mInference done 16/28. Dataloading: 0.0009 s/iter. Inference: 6.1198 s/iter. Eval: 15.3665 s/iter. Total: 21.4878 s/iter. ETA=0:04:17\n",
      "\u001b[32m[07/19 15:10:33 d2.evaluation.evaluator]: \u001b[0mInference done 17/28. Dataloading: 0.0009 s/iter. Inference: 5.9383 s/iter. Eval: 15.7756 s/iter. Total: 21.7155 s/iter. ETA=0:03:58\n",
      "\u001b[32m[07/19 15:11:11 d2.evaluation.evaluator]: \u001b[0mInference done 18/28. Dataloading: 0.0009 s/iter. Inference: 6.8694 s/iter. Eval: 16.1693 s/iter. Total: 23.0404 s/iter. ETA=0:03:50\n",
      "\u001b[32m[07/19 15:11:59 d2.evaluation.evaluator]: \u001b[0mInference done 19/28. Dataloading: 0.0009 s/iter. Inference: 7.9552 s/iter. Eval: 16.8373 s/iter. Total: 24.7942 s/iter. ETA=0:03:43\n",
      "\u001b[32m[07/19 15:13:05 d2.evaluation.evaluator]: \u001b[0mInference done 20/28. Dataloading: 0.0010 s/iter. Inference: 9.6344 s/iter. Eval: 17.8755 s/iter. Total: 27.5117 s/iter. ETA=0:03:40\n",
      "\u001b[32m[07/19 15:13:21 d2.evaluation.evaluator]: \u001b[0mInference done 21/28. Dataloading: 0.0010 s/iter. Inference: 9.5508 s/iter. Eval: 17.2449 s/iter. Total: 26.7974 s/iter. ETA=0:03:07\n",
      "\u001b[32m[07/19 15:13:29 d2.evaluation.evaluator]: \u001b[0mInference done 23/28. Dataloading: 0.0010 s/iter. Inference: 8.8398 s/iter. Eval: 15.4676 s/iter. Total: 24.3092 s/iter. ETA=0:02:01\n",
      "\u001b[32m[07/19 15:13:36 d2.evaluation.evaluator]: \u001b[0mInference done 24/28. Dataloading: 0.0009 s/iter. Inference: 8.6642 s/iter. Eval: 14.7315 s/iter. Total: 23.3974 s/iter. ETA=0:01:33\n",
      "\u001b[32m[07/19 15:13:45 d2.evaluation.evaluator]: \u001b[0mInference done 26/28. Dataloading: 0.0009 s/iter. Inference: 8.1767 s/iter. Eval: 13.4127 s/iter. Total: 21.5910 s/iter. ETA=0:00:43\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:07:38.858396 (19.950365 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:02:55 (7.633240 s / iter per device, on 1 devices)\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to bloc_segmentation\\mask_rcnn_R_101_FPN_3x\\2024-07-19-13-02-30\\evaluation\\6000_iter\\coco_instances_results.json\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.04 seconds.\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.647\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.591\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 55.190 | 70.671 | 64.746 | 0.000 | 8.089 | 58.449 |\n",
      "Loading and preparing results...\n",
      "DONE (t=0.05s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[07/19 15:13:51 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
      "\u001b[32m[07/19 15:13:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.57 seconds.\n",
      "\u001b[32m[07/19 15:13:52 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[07/19 15:13:52 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.562\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.647\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.597\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.116\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n",
      "\u001b[32m[07/19 15:13:52 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 56.169 | 70.832 | 64.712 | 0.000 | 7.801 | 59.666 |\n",
      "OrderedDict([('bbox', {'AP': 55.18954697678623, 'AP50': 70.67133700743773, 'AP75': 64.7459166897313, 'APs': 0.0, 'APm': 8.088904556090283, 'APl': 58.448688318755494}), ('segm', {'AP': 56.16906835496066, 'AP50': 70.8319740039481, 'AP75': 64.71155394251323, 'APs': 0.0, 'APm': 7.801053789589485, 'APl': 59.6662407295518})])\n"
     ]
    }
   ],
   "source": [
    "evaluation_path = os.path.join(OUTPUT_DIR_PATH, 'evaluation', f'{MAX_ITER}_iter')\n",
    "os.makedirs(evaluation_path, exist_ok=True)\n",
    "evaluator = COCOEvaluator(\"bloc_segmentation_test\", output_dir=evaluation_path)\n",
    "val_loader = build_detection_test_loader(cfg, \"bloc_segmentation_test\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision without data augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_PATH = r\"C:\\Users\\Jalil\\Desktop\\PROJECTS\\Vrak3D\\training\\bloc_segmentation\\mask_rcnn_R_101_FPN_3x\\2024-07-19-13-02-30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(WEIGHTS_PATH, 'model_final.pth')\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.552\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.707\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.647\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.081\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.114\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.591\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.627\n",
      "Loading and preparing results...\n",
      "DONE (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.562\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.708\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.647\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.078\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.597\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.116\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.596\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.084\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.633\n",
      "OrderedDict([('bbox', {'AP': 55.18954697678623, 'AP50': 70.67133700743773, 'AP75': 64.7459166897313, 'APs': 0.0, 'APm': 8.088904556090283, 'APl': 58.448688318755494}), ('segm', {'AP': 56.16906835496066, 'AP50': 70.8319740039481, 'AP75': 64.71155394251323, 'APs': 0.0, 'APm': 7.801053789589485, 'APl': 59.6662407295518})])\n"
     ]
    }
   ],
   "source": [
    "evaluation_path = os.path.join(WEIGHTS_PATH, 'evaluation', f'{MAX_ITER}_iter')\n",
    "os.makedirs(evaluation_path, exist_ok=True)\n",
    "evaluator = COCOEvaluator(\"bloc_segmentation_test\", output_dir=evaluation_path)\n",
    "val_loader = build_detection_test_loader(cfg, \"bloc_segmentation_test\")\n",
    "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
